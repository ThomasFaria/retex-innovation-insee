\subsection{Freins à l'innovation}
\begin{itemize}
    \item Thème général : donner de l'autonomie
    \item Limites du poste de travail : littérature sur scaling horizontal / vertical
    \item Observation commune aux différents INS :
    \begin{itemize}
        \item Insee / SSM : homogénéité des parcours, pourtant grande diversité d'infra, de moyens DSI $\rightarrow$ difficulté à partager des environnements, des formations $\rightarrow$ idée de fournir une ``sandbox'', un commun technologique (2020) [NB : dans la continuité, sandbox à l'échelle européenne via le one-stop-shop (2024)]
        \item Visions/incitations différentes DSI/statisticien $\rightarrow$ sécurité avant le fonctionnel
    \end{itemize}
    \item Inspirations : DevOps, DataOps
\end{itemize}

\subsection{Innovation technologique}

Bearing in mind these limitations, our objective was to develop a data platform empowering statisticians with greater freedom for innovation. To achieve this, we delved into the evolving data ecosystem, identifying two significant trends with the potential to overcome the aforementioned limitations. The first trend signals a move away from traditional big data architectures towards more modular, decoupled structures. The second trend highlights containerization technology as a means to enhance the autonomy of statisticians.

Over the last decade, the landscape of big data has dramatically transformed. Following the publication of Google's seminal papers that introduced the MapReduce paradigm \cite{ghemawat2003google, dean2008mapreduce}, Hadoop-based systems rapidly became the reference architecture of the big data ecosystem, celebrated for their capability to manage extensive datasets through the use of distributed computing. The inception of Hadoop marked a revolutionary step, enabling organizations to process and analyze data at an unprecedented scale. Basically, Hadoop provided companies with all-rounded capabilities for big data analytics : tools for ingestion, data storage (HDFS), and computing capacities (Spark, among others) \cite{dhyani2014big}, thus explaining its rapid adoption across industries.

In the late 2010's, Hadoop-based architectures have experienced a clear decline in popularity as the industry shifted toward more flexible, decoupled architectures. In traditional Hadoop environments, storage and compute were co-localized by design : if the source file is distributed across multiple servers (horizontal scaling), each section of the source file is directly processed on the machine hosting that section, so as to avoid network transitions between servers. In this paradigm, scaling the architecture often meant a linear increase in both compute and storage, regardless of the actual demand. In a recent article provocatively titled "Big Data is Dead"\footnote{\url{https://motherduck.com/blog/big-data-is-dead/}}, Jordan Tigani, one of the founding engineers behind Google BigQuery, explains why this model doesn't fit the reality of most data-centric organizations. First, because in practice, "data sizes increase much faster than compute sizes". While the amount of data generated and thus needing to be stored may grow linearly over time, it is generally the case that we only need to query the most recent portions of it, or only some columns and/or groups of rows. Besides, Tigani points out that "the big data frontier keeps receding" : advancements in server computing capabilities and declining hardware costs mean that the number of workloads that don't fit on a single machine - a simple yet effective definition of big data - has been continually decreasing. As a result, by properly separating storage and compute functions, even substantial data processing jobs may end up using "far less compute than anticipated [...] and might not even need to use distributed processing at all".

These insights strongly align with our own observations at Insee in recent years. As a use case of using big data infrastructures to improve statistical processes, a team at Insee set up a Hadoop cluster as an alternative architecture to the one already in use to process sales receipt data in the context of computing the consumer price index. An acceleration of data processing operations by up to a factor of 10 was achieved, for operations that previously took several hours to perform \cite{leclair2019utiliser}. Despite this great increase in performance, this type of architecture were not reused in subsequent projects for several reasons. Firstly, the architecture proved to be expensive and complex to maintain, necessitating specialized technical expertise rarely found within NSOs \cite{vale2015international}. More crucially, we noticed that the needs of recent innovative statistical projects carried out at Insee were very much in line with Tigani's observations. The bottleneck for these projects was generally on the side of computational needs - such as the need for GPUs to train or simply use deep-learning models - rather than storage capacity. Furthermore, although these projects could still deal with substantial data volumes, we observed that effective processing could be achieved using conventional software tools (R, Python) on single-node systems by leveraging recent promising tools from the data ecosystem. First, by using efficient formats to store the data such as Apache Parquet \cite{parquet2013}, which properties (columnar storage \cite{abadi2013design}, optimisation for the "write once, read many" (WORM) paradigm, ability to partition data, etc.) make it particularly suited to analytical tasks such as those generally performed in official statistics \cite{abdelaziz2023optimizing}. Second, by performing computations using in-memory computation frameworks such as Apache Arrow \cite{arrow2016} or DuckDB \cite{raasveldt2019duckdb}, that are also based on columnar representation - thus working in synergy with Parquet files - and implementing various optimizations (predicate pushdown, projections pushdown) to limit computations to data effectively needed, enabling much larger-than-memory data processing.

The advent of cloud technologies has been instrumental in facilitating the shift towards decoupled data architectures. Containerization, in particular, encapsulates applications in self-contained environments, ensuring consistency across development, testing, and production. This technology, coupled with orchestrators like Kubernetes, allows for dynamic resource allocation and scaling, reflecting the real-time demands of data processing tasks. Object storage further complements this architecture, offering highly scalable, durable, and cost-effective solutions for data storage that traditional file systems struggle to match. 

The rise of containerization also highlights a broader trend toward greater autonomy and agility in software development and deployment, as advocated by the DevOps approach. By abstracting the application from the underlying infrastructure, developers gain the freedom to innovate and iterate rapidly, without being bogged down by environment inconsistencies or deployment complexities. 






Observation : convergence d'éco-sytèmes.

Axe : big data is dead $\rightarrow$ architecture découplage.
\begin{itemize}
    \item Transition éco big data $\rightarrow$ éco découplage : co-localisation plus très justifiée
    \item Stockage objet
    \item Infra BD tradi très spécialisées (calcul distribué). Aujourd'hui avec ML etc cas d'usages bcp plus diversifiés $\rightarrow$ outils d'automatisation, MLOPS, GPUs
    \item Insee : déjà culture fichier SAS + volumétries limitées $\rightarrow$ sauté l'étape BDD (cf. big data is dead)
\end{itemize}

Axe : conteneurisation comme moyen d'autonomisation.
\begin{itemize}
    \item Conteneurisation = light virtualization vs. VM
    \item Tendance DevOps $\rightarrow$ DataOps, MLOps
    \item Reproductibilité des traitements
\end{itemize}