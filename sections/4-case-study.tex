\section{Case-study : deploying a machine learning model into production following MLOps principles}

This chapter aims, through a concrete example, to illustrate how INSEE managed to deploy its first machine learning model into production. It will delve into the MLOps approach that this project strived to adhere to as much as possible, focusing on the various technologies and infrastructures that were employed. This initial production deployment, while successful, faced various challenges, whether technical or organizational, and we will endeavor to discuss them and propose solutions wherever possible. The idea is to illustrate the development of this project as transparently as possible, without claiming it to be the definitive approach. The entire project is available in open source\footnote{\url{https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories}} and remains under active development.

\subsection{Context and motivations}

Coding tasks are common operations for all national statistical institutes and can sometimes be challenging due to the size of certain nomenclature. At INSEE, a highly sophisticated coding tool called Sicore was developed in the 1990s to perform various classifications. Sicore uses a reference file that can be considered as a training file, which serves as examples of codings. The label to be coded is compared to the labels contained in the training file, and when the label is recognized, the associated code is assigned. When the label is not recognized, it must be manually classified by an INSEE agent. Two main reasons drove the experimentation of new coding methods. Firstly, there was an internal change with the redesign of the Sirene registry, which lists all companies in France and assigns them a unique identifier, the Siren number, for use by public institutions, notably to improve the daily management of the registry for INSEE agents and to reduce waiting times for companies. Additionally, at the national level, the government launched a one-stop shop for business formalities, allowing more flexibility for business owners in describing their main activities.

The initial testing exercises revealed that Sicore was no longer the suitable tool for performing NACE classification, as only 30\% of tasks were being automatically coded. The teams working on the Sirene registry were already overwhelmed with numerous changes, making it unrealistic to further increase their workload with manual reclassification, which is both time-consuming and unstimulating. Therefore, in May 2022, the decision was made to experiment with new methods for performing this classification task, with the aim of using this method in production by January 1, 2023, the launch date of the new Sirene registry, if successful.

This choice of innovation was not initially a voluntary decision but rather a necessity, given that the current state of the process could not remain unchanged. Therefore, all decisions made during this project were taken considering these temporal and organizational constraints. The aim is to present these various strategic choices that we made at INSEE while bearing in mind that they may not be applicable or advisable in all organizations.

Three stakeholders were involved in this project: the business team responsible for managing the Sirene registry, the IT team developing software related to the registry's operation, and the \textit{"innovation"} team tasked with implementing the new coding tool. The latter team is the INSEE Lab, which was created in 2017 with the objective of providing support to other teams on innovation topics to streamline their various projects.

\subsection{Démarrage du projet comme les projets expérimental et prise en compte des contraintes}


The project we aim to implement is a standard natural language classification problem. Indeed, starting from a textual description, we want to predict the class associated with it in the NACE Rev. 2 nomenclature. This nomenclature has the particularity of being hierarchical and containing 5 different levels\footnote{Actually, there are 5 different levels in France but only 4 at the European level.}: section, division, group, class, and subclass. In total, 732 subclasses exist, which is the level at which we aim to perform our classification. Table \ref{tab:nace-nomenclature} summarizes this hierarchical structure with an example.

\begin{table}[htbp]
    \centering
    \begin{tabular}{llll}
    \textbf{Level} & \textbf{NACE} & \textbf{Title} & \textbf{Size} \\ \hline
    Section & H & Transportation and storage & 21 \\ \hline
    Division & 52 & Warehousing and support activities for transportation & 88 \\ \hline
    Group & 522 & Support activities for transportation & 272 \\ \hline
    Class & 5224 & Cargo handling & 615 \\ \hline
    \textbf{Subclass} & \textbf{\textcolor{red}{5224A}} & \textbf{Harbour handling} & \textbf{\textcolor{red}{732}} \\ 
    \end{tabular}
    \caption{NACE Nomenclature}
    \label{tab:nace-nomenclature}
    \end{table}


With the establishment of the one-stop shop, business owners can now freely draft their activity descriptions. As a result, the labels received by INSEE are very different from the harmonized labels that were previously received. Therefore, it was decided to work with machine learning models that have proven their effectiveness in the literature. This represents a significant paradigm shift from INSEE's perspective, as no machine learning model has ever been deployed into production. In fact, the initial years of the INSEE Lab were characterized by a multitude of experiments on various subjects without ever transitioning to production. Nevertheless, all these experiments were valuable and accelerated the project through the gained experience. This project thus marked the first instance where the challenges of production deployment were considered from the outset, guiding numerous methodological and technical choices. As such, several points had to be agreed upon to facilitate coordination among the various stakeholders, and several strategic choices had to be made from the outset, including the working infrastructure, work methods, and the type of model to be used for such a project. The goal was to accommodate the constraints of each team and reconcile their needs.

\subsubsection{Infrastructure de travail}

In a machine learning project, the choice of development infrastructure is central. Working on a modular infrastructure is crucial in machine learning projects due to the diversity of tasks to be performed, such as data collection, preprocessing, modeling, evaluation, inference, monitoring, among others. This allows for easy replacement or updating of components without disrupting the entire workflow pipeline. As elucidated in the preceding chapter, traditional Big Data infrastructures often prove too rigid and specialized, failing to adequately cater to the diverse demands of various projects. Therefore, we decided to use a more modular infrastructure that better addresses the needs of machine learning projects by leveraging the most appropriate technologies for each stage of our pipeline. This primarily allows us to take advantage of the latest technological advancements without being limited by a predefined architecture, as innovations in machine learning progress rapidly. We exclusively utilized open-source cloud technologies available in the SSP Cloud catalog, an instance of the Onyxia software developed by INSEE. This platform, based on Kubernetes, offers flexible and scalable container management, enabling easy scaling of services as needed. As we will see in the following sections of this chapter, we used various tools for each stage of our pipeline: MinIO, Vault, MLflow, ArgoCD, Argo Workflows, Label Studio, Vscode, etc. The decision to opt for the SSP Cloud was also motivated by the ongoing implementation of a private instance of Onyxia on the production servers at INSEE. This initiative is poised to streamline the connection between production and development environments in the long run.

While the utilization of SSP Cloud has proven to be the right solution for us, it does come with its set of considerations. Working on SSP Cloud implies operating in ephemeral environments, thus ensuring the proper backup of code and data is imperative. Thankfully, SSP Cloud provides a file storage solution through the use of MinIO, which addresses this requirement. Although Onyxia simplifies initial setup and usage, it may require a bit of a learning curve for new users. However, given the widespread adoption of Amazon's S3 storage system in cloud computing, investing time in understanding it proves beneficial for any data scientist. Furthermore, comprehensive documentation is available at the following address: \url{https://inseefrlab.github.io/docs.sspcloud.fr/docs/en/storage.html}.  Similarly, regular versioning of code is essential. We've opted for Git and Github for source code management, enabling seamless collaboration among our teams. While Git may require some initial learning, it's a crucial tool for anyone working on data science projects. To facilitate its adoption within INSEE, the innovation teams developed an internal training session on Git usage. For specific documentation related to SSP Cloud, it can be found \href{https://inseefrlab.github.io/docs.sspcloud.fr/docs/en/version-control.html}{here}. 

As for data privacy, while SSP Cloud is secure, it doesn't guarantee complete confidentiality. However, since we solely deal with open data, this hasn't posed any issues for us. At the time, the internal Onyxia instance at INSEE was not available yet.

\subsubsection{Méthodes de travail}

As mentioned earlier, our working infrastructure essentially required us to use Git to version our code. However, regardless of your setup, knowledge and usage of Git are essential prerequisites for any machine learning project. Another significant choice was made at the project's outset: the programming language to use. Currently, INSEE is undertaking a large-scale project to migrate all SAS code to the open-source R language. While we could have followed suit to align our projects with other INSEE initiatives, we opted for Python. Without getting into the futile debate over the superiority between R and Python, it's generally accepted that the majority of the machine learning ecosystem leans towards Python. That's why we chose this language. However, this decision wasn't made lightly, as it means our three stakeholders primarily work with three different programming languages: Java for IT teams, R for business teams, and Python for innovation teams. This diversity can pose a significant challenge to collaboration among the three teams, and we'll explain how we've attempted to overcome this hurdle.

In this project, we deliberately favored the use of scripts over notebooks, even though we're accustomed to using the latter in other projects. With a focus on production deployment, where our main goal was to ensure code scalability and efficient maintenance, notebooks present several significant limitations, including:

\begin{enumerate}
    \item Defining and making all objects (functions, classes, and data) available in the same file, thereby complicating long-term code maintenance.
    \item Limited potential for automating ML pipelines.
    \item Notebooks' tendency to encourage code duplication and marginal modifications rather than using functions, making the code less modular and harder to maintain.
    \item Lack of extensions to implement best practices, such as linters, making it challenging to apply code quality standards.
    \item The costly transition to production with notebooks, whereas well-structured scripts are easier to deploy.
    \item Major versioning challenges with Git, as notebooks are essentially large JSON files, making it difficult to identify code changes and thus collaborate among team members.
\end{enumerate}

Moreover, by open-sourcing all our code, we've committed to following community standards by documenting our code and using formatters such as Ruff (\url{https://github.com/astral-sh/ruff}).


\subsubsection{Méthodologie retenue}

When it came to selecting the methodology to adopt, we had to navigate through various constraints, with the most significant being the need for deployment on our production servers. Our model had to be lightweight enough to run efficiently on these servers, while also minimizing the computational resource overhead to ensure swift inference. Additionally, the model had to be compatible with a different language than the one it was trained in—specifically, Java, utilized on our production servers. This proved particularly challenging throughout our project, as it influenced our data preprocessing choices, necessitating simplicity wherever possible. We had to meticulously replicate the data processing performed in Python during training, thereby limiting the use of certain packages. Ultimately, each decision made had to ensure the model's compatibility and efficiency within a production environment while minimizing compromises on inference quality.

These constraints led us away from the most powerful language models at the start of the project, such as Transformer models, and instead directed us towards simpler natural language models, specifically, we opted for the fastText model \cite{joulin2016bag}. This choice was driven by its ability to address all previously mentioned constraints. The fastText model is incredibly fast to train, even from scratch, and inference doesn't require a GPU to be extremely rapid. Moreover, there exists a wrapper that enables reading fastText models in Java, which could be utilized by the IT teams on their machines, greatly facilitating the deployment of our models. Unfortunately, this wrapper, available on \href{https://github.com/vinhkhuc/JFastText}{GitHub}, is no longer maintained, posing serious security concerns, and thus became a temporary default choice for us. In addition to these technical arguments, the decision to use the fastText model was justified from a methodological standpoint. Firstly, the INSEE Lab teams had already completed several projects using the fastText model, leveraging the acquired knowledge to achieve initial results very quickly. While it may not have been a state-of-the-art language model, for our use case, the model yielded excellent performance results which, considering the time and human resource constraints, were more than sufficient to enhance the existing process. Finally, the model is inherently simple methodologically speaking, greatly simplifying communication and adoption within the various INSEE teams. 

The supervised classification model fastText relies on both a bag-of-words model to obtain embeddings and a classifier based on logistic regression. The bag-of-words approach involves representing a text as the set of vector representations of each of its constituent words. Thus, the embedding of a sentence depends on the embeddings of its words, for example, their sum or their average. In the case of supervised text classification, the embedding matrix and the classifier's coefficient matrix are learned simultaneously during training by gradient descent, minimizing an usual cross-entropy loss function. The specificity of the fastText model lies in embeddings being performed not only on words but also on word n-grams and character n-grams, providing more context and reducing biases due to spelling mistakes. The fastText model can be summarized by the following diagram:

% TODO: diagram fasttext

The model chosen after various trials is the fastText model \cite{joulin2016bag}, for several reasons:

\begin{enumerate}
    \item The innovation team had gained experience in using this model through several previous experiments.
    \item The performance obtained was very good.
    \item The model is very simple methodologically and quick to train.
    \item There is a Java wrapper available that allows reading fastText models. (share github ?)
    \item Once trained, the model is lightweight enough to be deployed on our production servers.
\end{enumerate}


rappeler les nouveaux enjeux pour les projets de ML (model versionning, logging parameters) 
L'utilisation du ssp cloud permet d'accéder à plusieurs logiciels tous interconnectés pour favoriser le developpement de projet de machine learning favorisant une approche MLOps
Objectif d'appliquer cette approche durant ce projet.

\subsection{MLflow as the cornerstone of the project}

Logiciel qui permet de suivre cette approche = MLflow et c'est dispo sur ssp cloud

\begin{itemize}
    \item Why Mlflow ?
    \item Projects
    \item Models
    \item Tracking server
    \item Model registry
\end{itemize}

schema de mlflow

\subsection{Embracing the power of Onyxia from training to deployment}

\begin{itemize}
    \item Distributing trainings with Argo workflows
    \item Deployment on the kubernetes cluster (freed from DSI) with fastAPI  $\rightarrow$ conteneurisation Docker
    \item Automatiser les déploiements avec argoCD
\end{itemize}

Environnement dev et production très proche $\rightarrow$ passage en prod facilité
\begin{itemize}
    \item Transmission d'une image
    \item Transmission d'une API
\end{itemize}

schema api deployment

\subsection{Monitoring of the model}

\begin{itemize}
    \item Enjeu du monitoring => indispensable
    \item monitorer l'IC dire que c'est notre variable d'ajustement
    \item data drift/ concept drift
    \item Pour APE : Création d'un dashboard (faire un super graphs qui récap tout)
    \item encore on utilise les trucs du datalab (argocd pour le déploiement, argoworkflow pour les cronjob quotidien)
\end{itemize}

\subsection{Annotation en continue}

\begin{itemize}
    \item Evaluer la performance en créant un fichier test golden standard -> intégré au dashboard
    \item Amélioration du jeu d'entrainement en corrigeant les erreurs
    \item passage en NAF2025 très bientôt gros enjeu
    \item tout ca réalisé sur le datalab avec LabelStudio
    \item Rappeler les problèmes rencontrés (faire comprendre aux équipes métiers que c'est ultra important pour améliorer la performance, nécessite ressources humaines importantes..)
\end{itemize}


\subsection{Gouvernance d'un projet de ML/ challenges}

ici dire tous les problèmes qu'on a eu (double preprocessing...)
parler de limportance de la communication avec les gestionnaire pour la confiance
dire que pour contrecarrer probleme de langage on a recruté un data scientist + les informaticien doivent se mettre à python

