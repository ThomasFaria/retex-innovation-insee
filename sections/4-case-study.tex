This chapter aims, through a concrete example, to illustrate how INSEE managed to deploy its first machine learning model into production. It will delve into the MLOps approach that this project strived to adhere to as much as possible, focusing on the various technologies and infrastructures that were employed. This initial production deployment, while successful, faced various challenges, whether technical or organizational, and we will endeavor to discuss them and propose solutions wherever possible. The idea is to illustrate the development of this project as transparently as possible, without claiming it to be the definitive approach. The entire project is available in open source at \url{https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories} and remains under active development.

\subsection{Context and motivations}

Coding tasks are common operations for all national statistical institutes and can sometimes be challenging due to the size of certain nomenclature. At INSEE, a highly sophisticated coding tool called Sicore was developed in the 1990s to perform various classifications. Sicore uses a reference file that can be considered as a training file, which serves as examples of codings. The label to be coded is compared to the labels contained in the training file, and when the label is recognized, the associated code is assigned. When the label is not recognized, it must be manually classified by an INSEE agent. Two main reasons drove the experimentation of new coding methods. Firstly, there was an internal change with the redesign of the Sirene registry, which lists all companies in France and assigns them a unique identifier, the Siren number, for use by public institutions, notably to improve the daily management of the registry for INSEE agents and to reduce waiting times for companies. Additionally, at the national level, the government launched a one-stop shop for business formalities, allowing more flexibility for business owners in describing their main activities.

The initial testing exercises revealed that Sicore was no longer the suitable tool for performing NACE classification, as only 30\% of tasks were being automatically coded. The teams working on the Sirene registry were already overwhelmed with numerous changes, making it unrealistic to further increase their workload with manual reclassification, which is both time-consuming and unstimulating. Therefore, in May 2022, the decision was made to experiment with new methods for performing this classification task, with the aim of using this method in production by January 1, 2023, the launch date of the new Sirene registry, if successful.




Un petit topo sur le contexte à l'Insee codification faite avec sicore et manuellement très couteuse et pas stimulante
Possible car équipe au pied du mur $\rightarrow$ Innovation possible mais pas voulue très contrainte en terme de timing

Faire un rappel méthodo de ce qu'on cherche à faire

\subsection{Démarrage du projet comme les projets expérimental et prise en compte des contraintes}

On a un passé avec beaucoup d'expérimentation mais pas vraiment de mise en prod. Un des rares projets où des le début l'enjeu de la mise en production a été prise en compte. concilier problématiques
informatiques et métiers.

ici on explique quel modèle on a utilisé et pourquoi fasttexte => java etccc

1er question, ou on peut travailler ? 
\begin{itemize}

    \item projet ML plusieurs tâches : modularité de l'infra + collaboration (git indispensable, stockage partagé)
    \item Illustration de la diversité des taches nécessaires dans un projet de ML et modularité indispensable de l'infra utilisée (reprendre infra Big Data trop spécifique et onyxia cool)
    \item dans notre cas données ouverte donc possibilité d'utiliser le ssp cloud
    \item Rappeler les contraintes/prérequis que cela impose : utilisation de Git n'est pas aisée et nécessite des formations (mise en place d'un cursus de formateurs pour former à l'Insee), sauvegarde des données sur MinIO et pas en  local car environnement éphémère
\end{itemize}

2eme question, comment travailler ?
\begin{itemize}
    \item Choix de langage de développement : python. Dire débat R et python, Insee est passé à du tout R mais ecosystème ML plutot python. Ne pas opposer les deux, ils sont complémentaire gnagna
    \item On travaille sur des notebook en local on obtient des bon résultats mais on arrive rarement à les mettre à l'echelle. 
    \item Rappeler tous les défauts des notebook pour la mise en prod. 
\end{itemize}


rappeler les nouveaux enjeux pour les projets de ML (model versionning, logging parameters) 
L'utilisation du ssp cloud permet d'accéder à plusieurs logiciels tous interconnectés pour favoriser le developpement de projet de machine learning favorisant une approche MLOps
Objectif d'appliquer cette approche durant ce projet.

\subsection{MLflow as the cornerstone of the project}

Logiciel qui permet de suivre cette approche = MLflow et c'est dispo sur ssp cloud

\begin{itemize}
    \item Why Mlflow ?
    \item Projects
    \item Models
    \item Tracking server
    \item Model registry
\end{itemize}

\subsection{Embracing the power of Onyxia from training to deployment}

\begin{itemize}
    \item Distributing trainings with Argo workflows
    \item Deployment on the kubernetes cluster (freed from DSI) with fastAPI  $\rightarrow$ conteneurisation Docker
    \item Automatiser les déploiements avec argoCD
\end{itemize}

Environnement dev et production très proche $\rightarrow$ passage en prod facilité
\begin{itemize}
    \item Transmission d'une image
    \item Transmission d'une API
\end{itemize}

\subsection{Monitoring of the model}

\begin{itemize}
    \item Enjeu du monitoring => indispensable
    \item data drift/ concept drift
    \item Pour APE : Création d'un dashboard (faire un super graphs qui récap tout)
    \item encore on utilise les trucs du datalab (argocd pour le déploiement, argoworkflow pour les cronjob quotidien)
\end{itemize}

\subsection{Annotation en continue}

\begin{itemize}
    \item Evaluer la performance en créant un fichier test golden standard -> intégré au dashboard
    \item Amélioration du jeu d'entrainement en corrigeant les erreurs
    \item passage en NAF2025 très bientôt gros enjeu
    \item tout ca réalisé sur le datalab avec LabelStudio
    \item Rappeler les problèmes rencontrés (faire comprendre aux équipes métiers que c'est ultra important pour améliorer la performance, nécessite ressources humaines importantes..)
\end{itemize}


\subsection{Gouvernance d'un projet de ML/ challenges}

