\section{Case-study : deploying a machine learning model into production following MLOps principles}

This chapter aims, through a concrete example, to illustrate how INSEE managed to deploy its first machine learning model into production. It will delve into the MLOps approach that this project strived to adhere to as much as possible, focusing on the various technologies and infrastructures that were employed. This initial production deployment, while successful, faced various challenges, whether technical or organizational, and we will endeavor to discuss them and propose solutions wherever possible. The idea is to illustrate the development of this project as transparently as possible, without claiming it to be the definitive approach. The entire project is available in open source\footnote{\url{https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories}} and remains under active development.

\subsection{Context and motivations}

Coding tasks are common operations for all national statistical institutes and can sometimes be challenging due to the size of certain nomenclature. At INSEE, a highly sophisticated coding tool called Sicore was developed in the 1990s to perform various classifications. Sicore uses a reference file that can be considered as a training file, which serves as examples of codings. The label to be coded is compared to the labels contained in the training file, and when the label is recognized, the associated code is assigned. When the label is not recognized, it must be manually classified by an INSEE agent. Two main reasons drove the experimentation of new coding methods. Firstly, there was an internal change with the redesign of the Sirene registry, which lists all companies in France and assigns them a unique identifier, the Siren number, for use by public institutions, notably to improve the daily management of the registry for INSEE agents and to reduce waiting times for companies. Additionally, at the national level, the government launched a one-stop shop for business formalities, allowing more flexibility for business owners in describing their main activities.

The initial testing exercises revealed that Sicore was no longer the suitable tool for performing NACE classification, as only 30\% of tasks were being automatically coded. The teams working on the Sirene registry were already overwhelmed with numerous changes, making it unrealistic to further increase their workload with manual reclassification, which is both time-consuming and unstimulating. Therefore, in May 2022, the decision was made to experiment with new methods for performing this classification task, with the aim of using this method in production by January 1, 2023, the launch date of the new Sirene registry, if successful.

This choice of innovation was not initially a voluntary decision but rather a necessity, given that the current state of the process could not remain unchanged. Therefore, all decisions made during this project were taken considering these temporal and organizational constraints. The aim is to present these various strategic choices that we made at INSEE while bearing in mind that they may not be applicable or advisable in all organizations.

Three stakeholders were involved in this project: the business team responsible for managing the Sirene registry, the IT team developing software related to the registry's operation, and the \textit{"innovation"} team tasked with implementing the new coding tool. The latter team is the INSEE Lab, which was created in 2017 with the objective of providing support to other teams on innovation topics to streamline their various projects.

\subsection{Démarrage du projet comme les projets expérimental et prise en compte des contraintes}


The project we aim to implement is a standard natural language classification problem. Indeed, starting from a textual description, we want to predict the class associated with it in the NACE Rev. 2 nomenclature. This nomenclature has the particularity of being hierarchical and containing 5 different levels\footnote{Actually, there are 5 different levels in France but only 4 at the European level.}: section, division, group, class, and subclass. In total, 732 subclasses exist, which is the level at which we aim to perform our classification. Table \ref{tab:nace-nomenclature} summarizes this hierarchical structure with an example.

\begin{table}[htbp]
    \centering
    \begin{tabular}{llll}
    \textbf{Level} & \textbf{NACE} & \textbf{Title} & \textbf{Size} \\ \hline
    Section & H & Transportation and storage & 21 \\ \hline
    Division & 52 & Warehousing and support activities for transportation & 88 \\ \hline
    Group & 522 & Support activities for transportation & 272 \\ \hline
    Class & 5224 & Cargo handling & 615 \\ \hline
    \textbf{Subclass} & \textbf{\textcolor{red}{5224A}} & \textbf{Harbour handling} & \textbf{\textcolor{red}{732}} \\ 
    \end{tabular}
    \caption{NACE Nomenclature}
    \label{tab:nace-nomenclature}
    \end{table}


With the establishment of the one-stop shop, business owners can now freely draft their activity descriptions. As a result, the labels received by INSEE are very different from the harmonized labels that were previously received. Therefore, it was decided to work with machine learning models that have proven their effectiveness in the literature. This represents a significant paradigm shift from INSEE's perspective, as no machine learning model has ever been deployed into production. In fact, the initial years of the INSEE Lab were characterized by a multitude of experiments on various subjects without ever transitioning to production. Nevertheless, all these experiments were valuable and accelerated the project through the gained experience. This project thus marked the first instance where the challenges of production deployment were considered from the outset, guiding numerous methodological and technical choices. As such, several points had to be agreed upon to facilitate coordination among the various stakeholders, and several strategic choices had to be made from the outset, including the working infrastructure, work methods, and the type of model to be used for such a project. The goal was to accommodate the constraints of each team and reconcile their needs.

\begin{itemize}

    \item projet ML plusieurs tâches : modularité de l'infra + collaboration (git indispensable, stockage partagé)
    \item Illustration de la diversité des taches nécessaires dans un projet de ML et modularité indispensable de l'infra utilisée (reprendre infra Big Data trop spécifique et onyxia cool)
    \item dans notre cas données ouverte donc possibilité d'utiliser le ssp cloud
    \item Rappeler les contraintes/prérequis que cela impose : utilisation de Git n'est pas aisée et nécessite des formations (mise en place d'un cursus de formateurs pour former à l'Insee), sauvegarde des données sur MinIO et pas en  local car environnement éphémère
    rust bonnes pratiques etc
\end{itemize}

\subsubsection{2eme question, comment travailler ?}
\begin{itemize}
    \item Choix de langage de développement : python. Dire débat R et python, Insee est passé à du tout R mais ecosystème ML plutot python. Ne pas opposer les deux, ils sont complémentaire gnagna
    \item On travaille sur des notebook en local on obtient des bon résultats mais on arrive rarement à les mettre à l'echelle. 
    \item Rappeler tous les défauts des notebook pour la mise en prod. 
\end{itemize}


\subsubsection{3eme question, quel modèle utilisé ?}

The model chosen after various trials is the fastText model \cite{joulin2016bag}, for several reasons:

\begin{enumerate}
    \item The innovation team had gained experience in using this model through several previous experiments.
    \item The performance obtained was very good.
    \item The model is very simple methodologically and quick to train.
    \item There is a Java wrapper available that allows reading fastText models. (share github ?)
    \item Once trained, the model is lightweight enough to be deployed on our production servers.
\end{enumerate}


rappeler les nouveaux enjeux pour les projets de ML (model versionning, logging parameters) 
L'utilisation du ssp cloud permet d'accéder à plusieurs logiciels tous interconnectés pour favoriser le developpement de projet de machine learning favorisant une approche MLOps
Objectif d'appliquer cette approche durant ce projet.

\subsection{MLflow as the cornerstone of the project}

Logiciel qui permet de suivre cette approche = MLflow et c'est dispo sur ssp cloud

\begin{itemize}
    \item Why Mlflow ?
    \item Projects
    \item Models
    \item Tracking server
    \item Model registry
\end{itemize}

\subsection{Embracing the power of Onyxia from training to deployment}

\begin{itemize}
    \item Distributing trainings with Argo workflows
    \item Deployment on the kubernetes cluster (freed from DSI) with fastAPI  $\rightarrow$ conteneurisation Docker
    \item Automatiser les déploiements avec argoCD
\end{itemize}

Environnement dev et production très proche $\rightarrow$ passage en prod facilité
\begin{itemize}
    \item Transmission d'une image
    \item Transmission d'une API
\end{itemize}

\subsection{Monitoring of the model}

\begin{itemize}
    \item Enjeu du monitoring => indispensable
    \item data drift/ concept drift
    \item Pour APE : Création d'un dashboard (faire un super graphs qui récap tout)
    \item encore on utilise les trucs du datalab (argocd pour le déploiement, argoworkflow pour les cronjob quotidien)
\end{itemize}

\subsection{Annotation en continue}

\begin{itemize}
    \item Evaluer la performance en créant un fichier test golden standard -> intégré au dashboard
    \item Amélioration du jeu d'entrainement en corrigeant les erreurs
    \item passage en NAF2025 très bientôt gros enjeu
    \item tout ca réalisé sur le datalab avec LabelStudio
    \item Rappeler les problèmes rencontrés (faire comprendre aux équipes métiers que c'est ultra important pour améliorer la performance, nécessite ressources humaines importantes..)
\end{itemize}


\subsection{Gouvernance d'un projet de ML/ challenges}

