\section{Introduction}
\label{sec:introduction}

In recent years, the European Statistical System (ESS) has committed to leverage non-traditional data sources in order to improve the process of statistical production, an evolution that is encapsulated by the concept of Trusted Smart Statistics \cite{ricciato2019trusted}. This dynamic is accompanied by innovations in the statistical processes, so as to be able to take advantage of the great potential of these new sources (greater timeliness, increased spatio-temporal resolution, etc.), but also to cope with their complexity or imperfections. At the forefront of these innovations are machine-learning methods and their promising uses in the coding and classification fields, data editing and imputation \cite{gjaltema2022high}. The multiple challenges faced by statistical institutes because of this evolution are addressed in the Bucharest Memorandum on Official Statistics in a Datafied Society (Trusted Smart Statistics), which predicts that "the variety of new data sources, computational paradigms and tools will require amendments to the statistical business architecture, processes, production models, IT infrastructures, methodological and quality frameworks, and the corresponding governance structures", and consequently invites the ESS to assess the required adaptations and prioritize them \cite{bucharest2018}.

In line with these recommendations, much work has been done in the context of successive projects at the European level in order to operationalize the use of non-traditional data sources in the production of official statistics. Within the scope of the ESSnet Big Data II project (2018-2020), National Statistical Offices (NSOs) have been working across a wide range of themes (online job vacancies, smart energy, tracking ships, etc.) in order to put together the building blocks for using these sources in actual production processes and identify their limitations \cite{essnetbigdata2}. However, while a substantial amount of work has been devoted to developing methodological frameworks \cite{descy2019towards, salgado2020mobile}, quality guidelines \cite{kowarik2022quality} as well as devising business architectures that make third-party data acquisition more secure \cite{ricciato2018processing}, not much has been said about the IT infrastructures and skills needed to properly deal with these new objects.

Big data sources, which are at the heart of Trusted Smart Statistics, have characteristics that, due to their volume, their velocity (speed of creation or renewal) or their variety (structured but also unstructured data, such as text and images), make them particularly complex to process. Besides, the "skills and competencies to automate, analyse, and optimize such complex systems are often not part of the traditional skill set of most National Statistical Offices" \cite{ashofteh2021data}. Not incidentally, an increasing number of public statisticians trained as data scientists have joined NSOs in recent years. Within its multiple meanings, the term “data scientist” reflects the increased involvement of statisticians in the IT development and orchestration of their data processing operations, beyond merely the design or validation phases \cite{davenport2012data}. However, based on our observations at Insee and other French statistical offices, the ability of these new data professionals to derive value from big data sources and/or machine learning methods is limited by several challenges.

A first challenge is related to the lack of proper IT infrastructures to tackle the new data sources that NSOs now have access to as well as the accompanying need for new statistical methods. For instance, big data sources require huge storage capacities and often rely on distributed computing frameworks to be processed \cite{liu2013computing}. Similarly, the adoption of new statistical methods based on machine learning algorithms often require IT capacities - in particular, GPUs (graphical processing units) - to massively parallelize computations \cite{saiyeda2017cloud}. Such resources are not readily available in traditional IT infrastructures. Furthermore, these new infrastructures generally require specific skills - especially to build and maintain them - that are not readily found in NSOs.

Another major challenge is related to the difficulty of transitioning from innovative experiments to production-ready solutions. Even when statisticians have access to development environments in which they can readily experiment, the step towards putting the application or model in production is generally very large. Such examples highlight the need to make statisticians more autonomous regarding the orchestration of their processings as well as fostering a more direct collaboration between teams, as advocated by the DevOps approach.

dev env : reproducible + collaborative

A third challenge is to foster reproducibility in official statistics production. This quality criterion involves devising processing solutions that can produce reproducible statistics on the one hand, and that can be shared with peers on the other hand.

We argue that these various challenges have an underlying common theme: fostering autonomy. In the era of data science, statisticians need to be able to carry out innovative experiments more freely so as to be able to innovate in the production of official statistics. To do so, they need to have access to development environment that are both scalable and equipped with modern computing resources, while fostering collaboration and reproducibility. Finally, they need to be more autonomous from IT regarding the access to these resources and their ability to tailor their development environments to their needs. They 

They need to be more autonomous from IT to tailor their development environments to their needs. the orchestration of their workflows so as to be able to prototype production-ready solutions.

Against that background, we developed Onyxia: an open source project that enables organizations to deploy data science platforms that foster innovation by giving statisticians more autonomy\footnote{\url{https://github.com/InseeFrLab/onyxia}}. This paper aims at describing the full thought process that gave birth to this project as well as exemplifying the way in which it empowers statisticians at Insee and became a cornerstone of our innovation strategy. In section~\ref{sec:principles}, we provide a detailed review of the recent evolutions of the data eco-system to explain how we came out with the relevant technologies to build a modern data science environment that fits the needs of statisticians. In particular, we show how cloud-native technologies, particularly containers and object storage, is key to building scalable and flexible environments that can enhance autonomy while promoting reproducibility in the production of official statistics. However, while cloud technologies have a lot of desirable properties that make them very relevant to build modern data science environements, they are also notoriously hard to configure and to use, which can make their widespread adoption harder. In section~\ref{sec:implementation}, we detail the core of the Onyxia project: how we made cloud technologies accessible to statisticians through a user-friendly interface and a catalogue of ready-to-use data science environments, while ensuring that no vendor lock-in situation happens for the institution or the users. We also show how providing an open-innovation instance of Onyxia, named SSP Cloud, greatly facilitated the adoption of these technologies and the new, improved development practices that it fosters. Finally, through the case study of the classification of French companies' activity, we show in section~\ref{sec:mlops} how these technologies were instrumental in putting a machine learning model in production at Insee following the industry best practices - namely, MLOps principles.


