In the context of the evolution towards the model of Trusted Smart Statistics \cite{ricciato2019trusted}, the European Statistical System (ESS) has committed in recent years to leverage non-traditional data sources in order to improve the process of statistical production. This evolution is accompanied by innovations in the statistical processes, so as to be able to take advantage of the great potential of these new sources (greater timeliness, increased spatio-temporal resolution, etc.), but also to cope with their complexity or imperfections. At the forefront of these innovations are machine-learning methods and their promising uses in the coding and classification fields, data editing and imputation \cite{gjaltema2022high}. The multiple challenges faced by statistical institutes because of this evolution are addressed in the Bucharest Memorandum on Official Statistics in a Datafied Society (Trusted Smart Statistics) \cite{bucharest2018}, which predicts that "the variety of new data sources, computational paradigms and tools will require amendments to the statistical business architecture, processes, production models, IT infrastructures, methodological and quality frameworks, and the corresponding governance structures", and consequently invites the ESS to assess the required adaptations and prioritize them.

Following these principles, ESS Net Big Data I \& II -> much work has been done in the areas of processes (collaboration with business to secure data), methodology (ex : covid), quality frameworks (ex : ). However, not much in the domain of IT infrastructures and skills. Not incidentally, an increasing number of public statisticians trained as data scientists have joined NSIs in recent years. However, these new profiles often find themselves isolated in national statistical systems, and their ability to derive value from big data sources and/or machine learning methods is limited by several challenges. 

A first challenge is related to the lack of proper IT infrastructures to tackle the new data sources that NSIs now have access to as well as the accompanying need for new statistical methods. For instance, big data sources require huge storage capacities and often rely on distributed computing frameworks to be processed, which generally cannot be provided by traditional IT infrastructures \cite{liu2013computing}. Similarly, the adoption of new statistical methods based on machine learning algorithms often require IT capacities (notably, graphical processing units - GPUs) to massively parallelize computations \cite{saiyeda2017cloud}.

Another challenge is to foster reproducibility in official statistics production. This quality criterion involves devising processing solutions that can produce reproducible statistics on the one hand, and that can be shared with peers on the other hand [4].

A third challenge is related to the difficulty of transitioning from innovative experiments to production-ready solutions. Even when statisticians have access to development environments in which they can readily experiment, the step towards putting the application or model in production is generally very large. Such examples highlight the need to make statisticians more autonomous regarding the orchestration of their processings as well as fostering a more direct collaboration between teams, as advocated by DevOps and DataOps approaches.
 
- Against that background, we argue that fostering autonomy blabla
- ref innovation plateformes blabla
- choix technologiques qui favorisent l'autonomie et la scalabilit√©
- make cloud resources easily available
- retex : insee + ssp
- MLOps case study to illustrate
