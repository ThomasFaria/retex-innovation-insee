Bearing in mind these limitations, our objective was to develop a data platform empowering statisticians with greater freedom for innovation. To achieve this, we delved into the evolving data ecosystem, identifying two significant trends with the potential to overcome the aforementioned limitations. The first trend signals a move away from traditional big data architectures towards more modular structures based on cloud technologies. The second trend highlights containerization technology as a means to enhance the autonomy of statisticians.

\subsection{Limitations of traditional big data architectures}

Over the last decade, the landscape of big data has dramatically transformed. Following the publication of Google's seminal papers that introduced the MapReduce paradigm \cite{ghemawat2003google, dean2008mapreduce}, Hadoop-based systems rapidly became the reference architecture of the big data ecosystem, celebrated for their capability to manage extensive datasets through the use of distributed computing. The inception of Hadoop marked a revolutionary step, enabling organizations to process and analyze data at an unprecedented scale. Basically, Hadoop provided companies with all-rounded capabilities for big data analytics : tools for ingestion, data storage (HDFS), and computing capacities (Spark, among others) \cite{dhyani2014big}, thus explaining its rapid adoption across industries.

In the late 2010's, Hadoop-based architectures have experienced a clear decline in popularity. In traditional Hadoop environments, storage and compute were co-localized by design : if the source file is distributed across multiple servers (horizontal scaling), each section of the source file is directly processed on the machine hosting that section, so as to avoid network transitions between servers. In this paradigm, scaling the architecture often meant a linear increase in both compute and storage, regardless of the actual demand. In a recent article provocatively titled "Big Data is Dead"\footnote{\url{https://motherduck.com/blog/big-data-is-dead/}}, Jordan Tigani, one of the founding engineers behind Google BigQuery, explains why this model doesn't fit the reality of most data-centric organizations anymore. First, because "in practice data sizes increase much faster than compute sizes". While the amount of data generated and thus needing to be stored may grow linearly over time, it is generally the case that we only need to query the most recent portions of it, or only some columns and/or groups of rows. Besides, Tigani points out that "the big data frontier keeps receding" : advancements in server computing capabilities and declining hardware costs mean that the number of workloads that don't fit on a single machine - a simple yet effective definition of big data - has been continually decreasing. As a result, by properly separating storage and compute functions, even substantial data processing jobs may end up using "far less compute than anticipated [...] and might not even need to use distributed processing at all".

These insights strongly align with our own observations at Insee in recent years. As a use case of using big data infrastructures to improve statistical processes, a team at Insee set up a Hadoop cluster as an alternative architecture to the one already in use to process sales receipt data in the context of computing the consumer price index. An acceleration of data processing operations by up to a factor of 10 was achieved, for operations that previously took several hours to perform \cite{leclair2019utiliser}. Despite this great increase in performance, this type of architectures were not reused in subsequent projects for several reasons. Firstly, the architecture proved to be expensive and complex to maintain, necessitating specialized technical expertise rarely found within NSOs \cite{vale2015international}. More crucially, we noticed that the needs of recent innovative statistical projects carried out at Insee were very much in line with Tigani's observations. The bottleneck for these projects was generally on the side of computational needs rather than storage capacity. Furthermore, although these projects could still deal with substantial data volumes, we observed that effective processing could be achieved using conventional software tools (R, Python) on single-node systems by leveraging recent promising tools from the data ecosystem. First, by using efficient formats to store the data such as Apache Parquet \cite{parquet2013}, which properties (columnar storage \cite{abadi2013design}, optimisation for the "write once, read many" (WORM) paradigm, ability to partition data, etc.) make it particularly suited to analytical tasks such as those generally performed in official statistics \cite{abdelaziz2023optimizing}. Second, by performing computations using in-memory computation frameworks such as Apache Arrow \cite{arrow2016} or DuckDB \cite{raasveldt2019duckdb}, that are also based on columnar representation - thus working in synergy with Parquet files - and implementing various optimizations (predicate pushdown, projections pushdown) to limit computations to data effectively needed, enabling much larger-than-memory data processing on usual, single-node machines.

% TODO : figure to illustrate columnar representation and note to explain why it's especially suited to analytical processing such as those routinely performed in official statistics

\subsection{Embracing cloud-native technologies}

Against that background, the industry has shifted in recent years toward more flexible, decoupled architectures. The advent of cloud technologies has been instrumental in facilitating this shift, because of several reasons. First, as opposed to the hadoop-dominated era, network latency is much less of an issue today, so the "on-premise and co-located storage and compute" paradigm is not relevant anymore. Besides, we are observing an evolution that some have described as going from "big data to flexible data" : the infrastructures that process data still need to handle large volumes, but they need before all to be flexible in various ways : in the structure of the data (from structured, tabular to unstructured such as text, images, etc.), in its portability (the data should be deployable in multi-cloud and hybrid cloud environments seamlessly) and in the variety of the workloads that will use it (parallelized computations, deep-learning models that require GPUs, automation tools to deploy and monitor applications, etc.) \cite{barua2021data}. In recent years, two technologies have emerged in the data eco-system as the de-facto standard to enable this flexibility in cloud environments : containerization and object storage. 

In a cloud environment, the computer of the user becomes a simple access point to perform computations on a central infrastructure. This enables both ubiquitous access to and scalability of the services, as it is easier to scale a central infrastructure — usually horizontally, i.e. by adding more servers. However, such centralized infrastructures have two well-identified limitations that need to be dealt with : the competition between users in access to physical resources and the need to properly isolate deployed applications. The choice of containerization is fundamental as it tackles these two issues \cite{bentaleb2022containerization}. Fundamentally, a container is a logical grouping of resources that makes it possible to encapsulate an application, its libraries and other system dependencies, in a single package. By creating “bubbles” specific to each service, containers thus guarantee application isolation while remaining lightweight, as they share the support operating system with the host machine - contrary to virtual machines (see. graph X). In order to manage multiple containerized applications in a systematic way, containerized infrastructures generally rely on an orchestrator software - the most prominent one being Kubernetes - which optimises the allocation of physical resources for a set of containers and facilitates their connection. Interestingly, this property makes it possible to handle very large volumes of data in a distributed way : containers break down big data processing operations into a multitude of small tasks, organized by the orchestrator, thus minimizing the required resources while relaxing the co-location constraint that characterizes hadoop-based architectures.

% TODO : figure containers vs VM

The other fundamental choice in a data architecture is the nature of data storage. In the cloud ecosystem, object storage - thank's to Amazon's "S3" (Simple Storage Service) - has become the de-facto reference \cite{samundiswary2017object}. In this paradigm, files are stored as "objects" consisting of data, an identifier and metadata. This type of storage is optimized for scalability, as objects are not limited in size and the underlying technology enables cost-effective storage of (potentially very) large files. It is also instrumental in building a decoupled infrastructure such as discussed before : the data repositories - referred to as "buckets" - are directly searchable using standard HTTP requests through a standardized REST API. In a world where network latency is not a real bottleneck anymore, this means that storage and compute don't have to be on the same machines and in the same location, and can thus scale independently according to specific organization demands. Finally, object storage is a natural complement to architectures based on containerised environments for which it provides a persistence layer - containers being stateless by design - and easy connectivity without compromising security, or even with strengthened security compared with a traditional storage system \cite{mesnier2003object}.

\subsection{Technology as a means to enhance autonomy and reproducibility}

In the technical discussion above, we highlighted the technological choices that emerged from delving into the modern data ecosystem as fundamental principles for building a modern, scalable and flexible data architecture. Now, we want to provide a series of observations that illustrate how these choices are also highly relevant to the actual practice of official statistics.

The rise of containerization also highlights a broader trend toward greater autonomy and agility in software development and deployment, as advocated by the DevOps approach. By abstracting the application from the underlying infrastructure, developers gain the freedom to innovate and iterate rapidly, without being bogged down by environment inconsistencies or deployment complexities. 

- DevOps, DataOps principles
- Infrastructure as code
- users can both freely tailor services to their needs (programming language, system libraries, packages and their versions, etc.) while scaling their applications to the computing power and storage capacities it demands, e.g. by distributing computations over several containers
- Portability -> more fluid transitions between dev and production environments
- 

Besides scalability and autonomy, these architectural choices also foster reproducibility of statistical computations. This scientificity criterion is essential in official statistics. Fostering reproductibility in statistical production involves devising processing solutions that can produce reproducible statistics on the one hand, and that can be shared with peers on the other hand \cite{ntts2019reproducibility}. Traditional IT infrastructures — either a personal computer or a shared infrastructure with remote desktop access — fall short in this regard, as building a project or just computing a statistical indicator there generally involves a series of manual steps (installing system libraries, the programming language binary, projects packages, dealing with potentially conflicting versions, etc.) that can not be fully reproduced across projects. In comparison, containers are reproducible by design, as their build process involves defining precisely all the needed resources as a set of processing operations in a standardized manner, from the "bare machine" to the running application \cite{moreau2023containers}. Furthermore, these reproducible environments can be easily shared to peers as they can be readily published on open registries (for example, a container registry such as DockerHub) along to the source code of the application (for example, on a public software forge like GitHub or GitLab).
