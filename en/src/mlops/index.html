<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>index – SSP Cloud Datalab</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../src/discussion/index.html" rel="next">
<link href="../../src/implementation/index.html" rel="prev">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b02c3babb8b8c269817561ecc0d317ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles/styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../src/mlops/index.html">4 - MLOps</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">SSP Cloud Datalab</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-translate"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../../fr/">
            Français
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../../en/">
            English
            </a>
          </li>
      </ul>
    </div>
    <a href="../../src/pdf-en/index.pdf" title="PDF" class="quarto-navigation-tool px-1" aria-label="PDF"><i class="bi bi-file-pdf-fill"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/introduction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/principles/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Principles</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/implementation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Implementations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/mlops/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4 - MLOps</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/discussion/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Discussion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-mlops" id="toc-sec-mlops" class="nav-link active" data-scroll-target="#sec-mlops"><span class="header-section-number">4</span> Case-study: using MLOps to improve NACE classification</a>
  <ul class="collapse">
  <li><a href="#improving-the-nace-classification-process-using-ml-methods" id="toc-improving-the-nace-classification-process-using-ml-methods" class="nav-link" data-scroll-target="#improving-the-nace-classification-process-using-ml-methods"><span class="header-section-number">4.1</span> Improving the NACE classification process using ML methods</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation"><span class="header-section-number">4.1.1</span> Motivation</a></li>
  <li><a href="#classification-task" id="toc-classification-task" class="nav-link" data-scroll-target="#classification-task"><span class="header-section-number">4.1.2</span> Classification task</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">4.1.3</span> Methodology</a></li>
  </ul></li>
  <li><a href="#a-production-first-approach-with-mlops" id="toc-a-production-first-approach-with-mlops" class="nav-link" data-scroll-target="#a-production-first-approach-with-mlops"><span class="header-section-number">4.2</span> A production-first approach with MLOps</a>
  <ul class="collapse">
  <li><a href="#sec-devops-mlops" id="toc-sec-devops-mlops" class="nav-link" data-scroll-target="#sec-devops-mlops"><span class="header-section-number">4.2.1</span> From DevOps to MLOps</a></li>
  <li><a href="#sec-principles-mlops" id="toc-sec-principles-mlops" class="nav-link" data-scroll-target="#sec-principles-mlops"><span class="header-section-number">4.2.2</span> Principles of MLOps</a></li>
  <li><a href="#implementation-with-mlflow" id="toc-implementation-with-mlflow" class="nav-link" data-scroll-target="#implementation-with-mlflow"><span class="header-section-number">4.2.3</span> Implementation with MLflow</a></li>
  </ul></li>
  <li><a href="#facilitating-iterative-development-with-cloud-technologies" id="toc-facilitating-iterative-development-with-cloud-technologies" class="nav-link" data-scroll-target="#facilitating-iterative-development-with-cloud-technologies"><span class="header-section-number">4.3</span> Facilitating iterative development with cloud technologies</a>
  <ul class="collapse">
  <li><a href="#a-flexible-development-environment" id="toc-a-flexible-development-environment" class="nav-link" data-scroll-target="#a-flexible-development-environment"><span class="header-section-number">4.3.1</span> A flexible development environment</a></li>
  <li><a href="#deploying-a-model" id="toc-deploying-a-model" class="nav-link" data-scroll-target="#deploying-a-model"><span class="header-section-number">4.3.2</span> Deploying a model</a></li>
  <li><a href="#building-an-integrated-pipeline" id="toc-building-an-integrated-pipeline" class="nav-link" data-scroll-target="#building-an-integrated-pipeline"><span class="header-section-number">4.3.3</span> Building an integrated pipeline</a></li>
  <li><a href="#sec-monitoring" id="toc-sec-monitoring" class="nav-link" data-scroll-target="#sec-monitoring"><span class="header-section-number">4.3.4</span> Monitoring a model in production</a></li>
  <li><a href="#sec-annotation" id="toc-sec-annotation" class="nav-link" data-scroll-target="#sec-annotation"><span class="header-section-number">4.3.5</span> Promoting continuous improvement of the model</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="sec-mlops" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Case-study: using MLOps to improve NACE classification</h1>
<p>This chapter aims, through a concrete example, to illustrate how Insee managed to deploy its first machine learning (ML) model into production. It provides an in-depth description of the MLOps approach that this project strived to adhere to, focusing on the various technologies that were employed. In particular, we highlight how cloud technologies were instrumental in building a solution iteratively and how Onyxia greatly facilitated this process by providing flexible development environments as well as tools to deploy and monitor models, promoting a continuous improvement loop. The entire project is available in open source<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and remains under active development.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<a href="">https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories</a></p></div></div><section id="improving-the-nace-classification-process-using-ml-methods" class="level2 page-columns page-full" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="improving-the-nace-classification-process-using-ml-methods"><span class="header-section-number">4.1</span> Improving the NACE classification process using ML methods</h2>
<section id="motivation" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">4.1.1</span> Motivation</h3>
<p>Coding tasks are common operations for NSOs and can sometimes be challenging due to the size of statistical classifications. At Insee, a sophisticated coding tool called Sicore was developed in the 1990s to perform various classification tasks <span class="citation" data-cites="meyer_sicore_1997">Meyer and Rivière (<a href="#ref-meyer_sicore_1997" role="doc-biblioref">1997</a>)</span>. It consists in a coding engine containing numerous deterministic rules which identify ground-truth labels. Each input label goes through these rules and when a ground-truth label is recognized, the associated code is assigned. When the label is not recognized, it must be manually classified by an Insee agent.</p>
<p>Two main reasons drove the experimentation of new coding methods.</p>
<p>Firstly, there was an internal change with the redesign of the French statistical business register, which lists all companies in France and assigns them a unique identifier used across public administrations. The main goals of this revamping were to improve the daily management of the registry for Insee agents and to reduce waiting times for companies. Additionally, at the national level, the government launched a one-stop shop for business formalities, allowing more flexibility for business owners in describing their main activities. Initial testing exercises revealed that Sicore was no longer the suitable tool for performing NACE classification, as only 30% of the input data were being automatically coded.</p>
<p>Three stakeholders were involved in this project: the business team responsible for managing the French statistical business register, the IT team developing softwares related to the register’s operation, and the innovation team responsible for implementing the new coding tool. The latter team is the SSP Lab, which was created in 2018 with the objective of providing support to other teams on innovation topics to streamline their various projects.</p>
</section>
<section id="classification-task" class="level3 page-columns page-full" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="classification-task"><span class="header-section-number">4.1.2</span> Classification task</h3>
<p>The project we describe consists in a standard NLP classification problem. Starting from a textual description of the activity, we want to predict the associated class in the NACE Rev.&nbsp;2 statistical classification. This classification has the particularity of being hierarchical and contains 5 different levels<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>: section, division, group, class, and subclass. In total, 732 subclasses are included in the classification, which is the level at which we aim to perform the classification. <a href="#tbl-nace-nomenclature" class="quarto-xref">Table&nbsp;1</a> provides an example of this hierarchical structure.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Actually, there are 5 different levels in France but only 4 at the European level.</p></div></div><div id="tbl-nace-nomenclature" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nace-nomenclature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: NACE Nomenclature
</figcaption>
<div aria-describedby="tbl-nace-nomenclature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 61%">
<col style="width: 2%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Niveau</strong></th>
<th><strong>NACE</strong></th>
<th><strong>Title</strong></th>
<th><strong>Size</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Section</td>
<td>H</td>
<td>Transportation and storage</td>
<td>21</td>
</tr>
<tr class="even">
<td>Division</td>
<td>52</td>
<td>Warehousing and support activities for transportation</td>
<td>88</td>
</tr>
<tr class="odd">
<td>Group</td>
<td>522</td>
<td>Support activities for transportation</td>
<td>272</td>
</tr>
<tr class="even">
<td>Class</td>
<td>5224</td>
<td>Cargo handling</td>
<td>615</td>
</tr>
<tr class="odd">
<td><strong>Subclass</strong></td>
<td><span class="red2"><strong>5224A</strong></span></td>
<td><strong>Harbour handling</strong></td>
<td><span class="orange"><strong>732</strong></span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>With the establishment of the one-stop shop, business owners now describe their activity description with a free-text field. As a result, the new labels are very different from the harmonized labels that were previously received. Therefore, it was decided to work with ML models, that are known to be effective on supervised text classification tasks <span class="citation" data-cites="li2022survey">Li et al. (<a href="#ref-li2022survey" role="doc-biblioref">2022</a>)</span>. This represents a significant paradigm shift from Insee’s perspective, as ML was not traditionally used in the actual production of official statistics. Besides, the perspective of putting the new model in production was considered from the outset, guiding numerous methodological and technical choices. As such, several strategic choices had to be made from the outset, including the methodology, the choice of a development environment consistent with the target production environment, and the adoption of collaborative work methods.</p>
</section>
<section id="methodology" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="methodology"><span class="header-section-number">4.1.3</span> Methodology</h3>
<p>Text classification from the free-text field provided by business owners is a complex task: the activity descriptions are relatively short and thus contain limited statistical information, can contain spelling mistakes, and often require domain knowledge to be properly classified. On such task, traditional text analysis methods such as count vectorization or TF-IDF often fall short whereas neural-network-based embedding methods tend to perform better <span class="citation" data-cites="li2022survey">Li et al. (<a href="#ref-li2022survey" role="doc-biblioref">2022</a>)</span>. However, such architectures often impose greater computational demands, as they are much larger and might require specific hardware such as GPUs to perform inference with acceptable latency. These constraints led us away from the most powerful language models, such as Transformer models, and instead directed us towards the fastText model <span class="citation" data-cites="joulin2016bag">Joulin et al. (<a href="#ref-joulin2016bag" role="doc-biblioref">2016</a>)</span>, a simpler embedding-based classifier. The fastText model is extremely fast to train, even from scratch, and inference does not require a GPU to achieve low latency time. Besides, the model yielded excellent performance results in our use-case that, considering the time and human resource constraints, were more than sufficient to enhance the existing process. Finally, the model’s architecture is relatively simple, simplifying communication and adoption within the various Insee teams.</p>
<p>The fastText model relies on a bag-of-words model to obtain embeddings and a classification layer based on logistic regression. The bag-of-words approach involves representing a text as the set of vector representations of each of its constituent words. The specificity of the fastText model compared to other embeddings-based approaches is that embeddings are not only computed on words but also on word n-grams and character n-grams, providing more context and reducing biases due to spelling mistakes. Then, the embedding of a sentence is computed as a function of the individual token embeddings, typically the average. In the case of supervised text classification, the embedding matrix and the classifier’s parameters are learned simultaneously during training by gradient descent, minimizing the cross-entropy loss function. <a href="#fig-fasttext" class="quarto-xref">Figure&nbsp;1</a> represents the full pipeline of operations performed by fastText on an example text input.</p>
<div id="fig-fasttext" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/fasttext.png" class="img-fluid figure-img" style="width:125.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Overview of the simplified process behind fastText classifications
</figcaption>
</figure>
</div>
</section>
</section>
<section id="a-production-first-approach-with-mlops" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="a-production-first-approach-with-mlops"><span class="header-section-number">4.2</span> A production-first approach with MLOps</h2>
<p>From the very onset of this project, the target was to go beyond mere experimentation and put the model in production. Besides, the goal with this pilot project was also to build a template for future ML projects at Insee. We thus strove to enforce best development practices from the very beginning of the project: following community standards for code quality, using scripts-based development over notebooks, building a modular package-like structure, etc. However, compared to traditional development projects, machine learning projects have specific features that make it necessary to apply a complementary set of best practices, gathered under the name of MLOps.</p>
<section id="sec-devops-mlops" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-devops-mlops"><span class="header-section-number">4.2.1</span> From DevOps to MLOps</h3>
<p>DevOps is a set of practices designed to foster collaboration between development (Dev) and operations (Ops) teams. The fundamental idea is to integrate the full lifecycle of a project in a single automated continuum. An important tool to achieve this continuity is CI/CD pipelines. With continuous integration (CI), each commit of new source code will trigger a pipeline of standardized operations, such as building the application, testing it and making it available as a release. Then, continuous deployment (CD) consists in tools to automate the deployment of the new code and limit manual intervention, while ensuring proper monitoring to guarantee process stability and security. This approach promotes a faster, continual release of necessary feature changes or additions. Furthermore, by encouraging collaboration between teams, DevOps also promotes a quicker cycle of innovation, allowing teams to address issues as they arise and incorporate feedback effectively throughout the project lifecycle.</p>
<p>The MLOps approach can be seen as an extension of DevOps, developed to address the specific challenges related to managing the lifecycle of ML models. Fundamentally, both DevOps and MLOps aim at building software in a more automated and robust manner. The main difference is that in MLOps, this software also has a machine learning component. Consequently, the lifecycle of the project gets more complex. The underlying ML model needs to be re-trained regularly, to avoid any loss of performance over time. Data ingestion must also be included in the pipeline, as new data may be used to improve performance. <a href="#fig-mlops-cycle" class="quarto-xref">Figure&nbsp;2</a> presents the steps of an ML project using the continuous representation traditionally seen in DevOps. This illustrates a fundamental principle of MLOps, the need for continuous improvement, described in more details in <a href="#sec-principles-mlops" class="quarto-xref">Section&nbsp;4.2.2</a>.</p>
<div id="fig-mlops-cycle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlops-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/mlops-cycle.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlops-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The MLOps approach promotes a continuous management of ML projects lifecycle
</figcaption>
</figure>
</div>
</section>
<section id="sec-principles-mlops" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-principles-mlops"><span class="header-section-number">4.2.2</span> Principles of MLOps</h3>
<p>MLOps is defined by a few core principles that are crucial for building production-grade and scalable ML applications. These principles are designed to address the specific challenges associated with ML workflows.</p>
<p>The most fundamental principle of MLOps is continuous improvement, reflecting the iterative nature of ML projects. In the experimentation phase, the model is developed using a training dataset, that generally differs from the target data in some respect. When a model is deployed in production, the new data that the model needs to perform prediction on can reveal insights about the model’s performance and potential shortcomings. These insights necessitate a return to the experimentation phase, where data scientists adjust or redesign their models to address any discovered issues or to improve accuracy. This principle thus emphasizes the importance of building a feedback loop that enables ongoing enhancements throughout the lifecycle of a model. Automation, particularly through the use of CI/CD pipelines, plays a crucial role in this process by making the transition between experimentation and production phases more continuous. Monitoring is also an essential part of this process: a model deployed in production needs to be continuously assessed so as to detect major drifts that may reduce the predictive performance of the model and thus necessitate further adjustments, such as re-training or fine-tuning the model.</p>
<p>Another major goal of MLOps is to promote reproducibility, ensuring that any ML experiment can be reliably reproduced with the same results. MLOps tools thus facilitate thorough logging of ML experiments, including data pre-processing steps, model hyperparameters, and training algorithms. Data, models, and code are versioned, enabling teams to revert to previous versions if an update does not perform as expected. Finally, these tools help to produce detailed specifications of the computing environment used to produce these experiments — such as versions of libraries — and often rely on containers to help replicate the same conditions in which the original model was developed.</p>
<p>Finally, MLOps aims at fostering collaborative work. ML-based projects generally involve a wider range of profiles: business units and data science teams on the one hand, developers and operations teams on the other. Like DevOps, MLOps thus emphasizes the need for a collaborative culture and to avoid working in silos. MLOps tools generally include collaborative features, such as centralized stores for ML models or ML features which facilitate the sharing of components between team members and limit redundancy.</p>
</section>
<section id="implementation-with-mlflow" class="level3 page-columns page-full" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="implementation-with-mlflow"><span class="header-section-number">4.2.3</span> Implementation with MLflow</h3>
<p>Numerous tools have been developed to implement the MLOps approach in actual projects. All of these frameworks aim at enforcing, in some form, the core principles described above. In this project, we chose to rely on a popular open-source framework named MLflow<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. This choice does not indicate any inherent superiority of MLflow over alternative software, but reflects a set of good properties associated with MLflow that made it a very relevant solution for our application. First, it covers the entire lifecycle of ML projects, while other tools may be more specialized in some parts of it. Second, it exhibits great interoperability as it is well-interfaced with popular ML libraries — such as PyTorch, Scikit-learn, XGBoost, etc. — and supports multiple programming languages — including Python, R, and Java, thus covering the spectrum of programming languages commonly used at Insee. Finally, it proved to be very user-friendly and thus encouraged adoption among the project members and facilitated continuous collaboration between them.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<a href="">https://github.com/MLflow/MLflow</a></p></div></div><p>MLflow provides a cohesive framework to operationalize MLOps principles effectively within ML projects. Data scientists can encapsulate their work in MLflow Projects that package together ML code and its dependencies, ensuring that each project is reproducible and can be consistently re-executed. A project relies on an MLflow Model, a standard format that is compatible with most ML libraries and offers a normalized way of serving the model, e.g.&nbsp;via an API. This interoperability and standardization are instrumental in supporting continuous improvement of the project, as models trained with a variety of packages can be readily compared or switched by one another without breaking any code. As experiments with various models progress, the Tracking Server logs detailed information about each run — hyperparameters, metrics, and outputs artifacts and metrics — which there again promotes reproducibility but also facilitates the model selection phase through a user-friendly interface. After this experimentation phase, selected models are integrated into the Model Registry, where they are versioned and staged for deployment. This registry serves as a centralized model store that enables the different project members or teams to collaboratively manage the lifecycle of the project. <a href="#fig-mlflow-components" class="quarto-xref">Figure&nbsp;3</a> shows the core components of MLflow and how they facilitate a more continuous and collaborative workflow inside an ML project.</p>
<div id="fig-mlflow-components" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlflow-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/mlflow-model-registry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlflow-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Core components of MLflow. Source: Databricks.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="facilitating-iterative-development-with-cloud-technologies" class="level2 page-columns page-full" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="facilitating-iterative-development-with-cloud-technologies"><span class="header-section-number">4.3</span> Facilitating iterative development with cloud technologies</h2>
<p>While continuous improvement is a fundamental principle of MLOps, it is also a very demanding one. In particular, it requires designing and building our project as an integrated pipeline whose various stages are mainly automated, from data ingestion to monitoring the model in production. In this context, iterative development is essential in order to build a minimum viable product that is then refined and improved over time. This section shows how cloud-native technologies, through the Onyxia project, were instrumental in building the project from the start as a collection of modular connected components, thus greatly enhancing the capacity for continuous refinement over time.</p>
<section id="a-flexible-development-environment" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="a-flexible-development-environment"><span class="header-section-number">4.3.1</span> A flexible development environment</h3>
<p>In a ML project, the flexibility of the development environment is essential. First, due to the diversity of tasks to be performed — data collection, preprocessing, modeling, evaluation, inference, monitoring, etc. Second, because ML is a fast-evolving field, it is preferable to build an ML app as a collection of modular components so as to be able to update components without disrupting the entire pipeline. As discussed in <a href="../../src/principles/index.html#sec-cloud-native">Section 2.2</a>, cloud-native technologies enable the creation of modular and scalable development environments.</p>
<p>However, as also discussed in <a href="../../src/implementation/index.html#sec-implementation">Section 3</a>, access to such resources is not enough. An ML project requires a wide variety of tools to comply with MLOps principles — data storage, interactive development environments to experiment freely, automation tools, monitoring tools, etc. While these tools can be installed on a Kubernetes cluster, making them available to data scientists in an integrated and pre-configured manner is essential to facilitate their adoption. Through its catalog of services and the automatic injection of configuration in the services, Onyxia enables building projects that rely on multiple cloud-native components that can communicate readily with each other.</p>
<p>The way model training was carried out for this project emphasizes the flexibility provided by Onyxia in the experimentation phase. All the code performing the training is written in Python in the context of a VSCode service. As personal S3 credentials are injected in each service at startup, the various users of the projects can interact directly with the training data stored on a S3 bucket on MinIO, Onyxia’s default object storage solution. All experiments performed for the model selection phase are logged on a shared instance of MLflow, which stores logged data on a PostgreSQL instance automatically launched on Kubernetes and artifacts (trained models and associated metadata) on MinIO. The model was trained using grid-search for hyperparameter tuning and evaluated through cross-validation, a combination that is known to provide a better evaluation of the generalization performance of the model but also requires a lot of computing resources, due to the combinatory nature of testing many hyperparameters combinations. In our case, we leveraged Argo Workflows, an open-source workflow engine designed to orchestrate parallel jobs on Kubernetes, each job being specified as an independent container. It then becomes straightforward to compare performances of the different trained models and select the best one using the comparison and visualization tools available in the UI of MLflow.</p>
<p>In summary, the training stage was made efficient and reproducible thanks to the use of numerous modularly connected components — a distinctive feature of cloud-native technologies — readily made available to data scientists by Onyxia.</p>
</section>
<section id="deploying-a-model" class="level3 page-columns page-full" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="deploying-a-model"><span class="header-section-number">4.3.2</span> Deploying a model</h3>
<p>Once candidate models have been optimized, evaluated and a best-performing model has been selected, the next step is to make it available to the application end users. Simply providing the trained model as an artifact or even just the code to train the model is not a convenient way to serve it, as it assumes that users have the resources, infrastructure, and knowledge required for training it under the same conditions. The goal is therefore to make the model available in a simple and interoperable manner, in the sense that it should be possible to query it with various programming languages. Furthermore, it should be possible for other applications to query the model in a programmatic way.</p>
<p>Against that background, we opted to serve the model through a REST API. This technology has become a standard way to serve ML models as they offer several benefits. First, they fit very well a cloud-oriented environment: similarly to the other components of our stack, it makes it possible to query the model using standard HTTP requests, which contributes to the modularity of the system. It also means that they are interoperable: as they rely on standards technologies for queries (HTTP requests) and responses (generally, a JSON-formatted string), they are mostly agnostic to the programming language used to request them. Finally, they offer great scalability because of their stateless design<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. As each request must contain all the information needed to understand and process the request, REST APIs can easily be duplicated on different machines to balance a challenging load — a process known as horizontal scaling.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Stateless design refers to a system architecture where each request from a client to the server must contain all the information needed to understand and process the request. This means that the server does not store any information about the client’s state between requests, allowing each request to be handled independently. This design simplifies scaling and enhances the robustness of the system, as any server can handle any request without relying on prior interactions.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;<a href="">https://fastapi.tiangolo.com</a></p></div></div><p>We developed the API serving the model with FastAPI<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, a fast and well-documented web framework for building APIs with Python. The API code and required software dependencies are encapsulated into a Docker image so that it can be deployed as a container on the Kubernetes cluster. An important benefit of using Kubernetes is the ability to scale the API — through the number of API pods effectively deployed — to the demand and provide automatic load-balancing. Upon startup, the API automatically retrieves the correct model from the MLflow model registry, which acts as a proxy from the actual artifact of the production model, stored on MinIO. Finally, as the application code is packaged using MLFlow’s standardized API — enabling for instance to integrate the pre-processing step directly to each API call — the inference code can remain mostly uniform regardless of the underlying ML framework used. This deployment process is summarized in <a href="#fig-api-datalab" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div id="fig-api-datalab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-api-datalab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/api-datalab.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-api-datalab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: A cloud-native approach to serve a ML Model using a REST API
</figcaption>
</figure>
</div>
</section>
<section id="building-an-integrated-pipeline" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="building-an-integrated-pipeline"><span class="header-section-number">4.3.3</span> Building an integrated pipeline</h3>
<p>The architecture we built at this stage already reflects some important principles of MLOps. The use of containerization to deploy the API as well as the use of MLflow to track the experiments while developing the model ensures reproducibility of the predictions. Using the central model registry provided by MLflow facilitates the management of the lifecycle of the models in a collaborative way. Furthermore, the modularity of our architecture leaves room for further improvement as modular components can be easily added or modified without breaking the structure of the application as a whole. As we shall see in subsequent sections, this property was essential in building the application iteratively, enabling to add a monitoring layer (<a href="#sec-monitoring" class="quarto-xref">Section&nbsp;4.3.4</a>) and an annotation component (<a href="#sec-annotation" class="quarto-xref">Section&nbsp;4.3.5</a>) to promote continuous improvement of the model.</p>
<p>However, the ability to refine the base architecture iteratively also requires more continuity in the process. At this stage, the deployment process involves several manual operations. For instance, adding a new feature to the API would require to build a new image, tag it, update the Kubernetes manifests used to deploy the API and enforce them on the cluster to replace the existing one with minimum downtime. Similarly, a change of the model served through the API would require a very simple modification of the code but several manual steps to update the version on the cluster. As a result, data scientists are not fully autonomous when it comes to prototyping and testing updated versions of the model or the API, which limit the potential for continuous improvement.</p>
<p>In order to automate this process, we built a CI/CD pipeline — a concept already presented in <a href="#sec-devops-mlops" class="quarto-xref">Section&nbsp;4.2.1</a> — integrating these various steps. <a href="#fig-ci-cd" class="quarto-xref">Figure&nbsp;5</a> illustrates our specific implementation of a CI/CD pipeline. Any change in the code of the API repository triggers a CI build process (implemented with GitHub Actions) of the associated docker image, which is then released on a public container registry (DockerHub). This image can then be fetched and deployed by the container orchestrator (Kubernetes), by specifying and applying manually new manifests to update the Kubernetes resources of the API. However, the downside of this approach is that it limits reproducibility of the deployment, since each resource is handled independently by the orchestrator, so that the lifecycle of the API deployment as a whole is not managed. To overcome this shortcoming, we integrate the deployment part in a CD pipeline based on the GitOps approach: the resources manifest of the API are stored on a Git repository. The state of this “GitOps” repository is monitored by a Kubernetes operator (ArgoCD), so that any change in the application manifests is directly propagated to the deployment on the cluster. In this integrated pipeline, the only action needed for the data scientist to trigger an update of the API is to change the tag of the API image indicating the version to be deployed.</p>
<div id="fig-ci-cd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/ci-cd.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: The CI/CD pipeline implemented in the project
</figcaption>
</figure>
</div>
</section>
<section id="sec-monitoring" class="level3 page-columns page-full" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="sec-monitoring"><span class="header-section-number">4.3.4</span> Monitoring a model in production</h3>
<p>Once the initial development phase of the project has been completed — including training, optimization, and deployment of the model to be served to users — it is crucial to understand that the data scientist’s responsibilities extend further. Traditionally, the role of the data scientist is often limited to training the model and selecting the model to deploy, with the deployment task being delegated to the IT department. However, a specificity of ML projects is that, once in production, the model has not yet reached the end of its lifecycle, and it must be continuously monitored to prevent undesirable performance degradation. Continuous monitoring of the deployed model is extremely important to ensure the conformity of results to expectations, anticipate changes in data, and iteratively improve the model.</p>
<p>The concept of monitoring can take on different meanings depending on the context of the involved team. For IT teams, it primarily involves verifying the technical effectiveness of the application, including aspects such as latency, memory consumption, or disk usage. Conversely, for data scientists or business teams, the focus is more on methodological monitoring of the model. However, real-time tracking of the performance of an ML model is often a complex task, given that ground truth is usually not known at the time of prediction. Therefore, it is common to use proxies to detect any signs of performance degradation. Two main types of degradation of an ML model are generally distinguished. The first one is data drifts, which occur when the data used during inference in production exhibits significant differences compared to the data used during training. The second one is concept drifts, which occur when a change in the statistical relationship between the features and the target variable is observed over time.</p>
<p>In the context of our project, the objective is to achieve the highest rate of correctly classified textual description while minimizing the number of textual description requiring manual intervention. Thus, our goal is to distinguish correct predictions from incorrect ones without prior access to ground truth. To accomplish this, we use a confidence index defined as the difference between the two highest confidence scores of the top results returned by the model. For a given textual description, if the confidence index exceeds a determined threshold, the textual description is automatically coded. Otherwise, the textual description is manually coded by an Insee agent. This manual coding task is still informed by the ML model: through an application that queries the API, the agent is shown the five most probable codes according to the model.</p>
<p>Defining the threshold for automatic coding of textual descriptions was thus crucial in this process, and involved making a trade-off between achieving a high automatic coding rate and maximizing coding performance. To monitor the behaviour of our model in production, we developed an interactive dashboard that enables visualization of several metrics of interest for the business teams. Among these metrics are the number of requests per day and the rate of automatic coding per day based on a given confidence index threshold. This visualization allows business teams to understand the rate of automatic coding they would have obtained if they had chosen different thresholds. This dashboard also represents the distribution of obtained confidence indices and compare temporal windows in order to check for changes in the distributions of predictions returned by the model<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Finally, confidence indices can be analysed at finer levels of granularity based on the aggregation level of the statistical classification, to determine which classes are most difficult to predict and which have more or less occurrences.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Such distribution changes are typically checked by computing statistical distances — such as the Bhattacharyya distance, the Kullback-Leibler divergence, the Hellinger distance — and/or by performing statistical tests — such as the Kolmogorov–Smirnov or the chi-squared test.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;Ideally, existing frameworks should be preferred over custom-made solutions to prioritize standardized routines. At the time of building this component of the pipeline, we found that existing, cloud-native frameworks for log analytics had important limitations. This constitutes an area of improvement for the project.</p></div></div><p><a href="#fig-full-architecture" class="quarto-xref">Figure&nbsp;6</a> shows the components that were added to the project architecture so as to provide the monitoring dashboard described above. First, we set up a simple Extract-Transform-Load (ETL) process in Python (second box of the bottom row), which fetches the API logs periodically and transforms them into partitioned Parquet files<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Then, we use the Quarto framework[^quarto] to build an interactive dashboard (third box of the bottom row). To compute the various metrics presented in the dashboard, the Parquet files are queried using the SQL language through the DuckDB engine. Like the API, the dashboard is built and deployed as a container on the Kubernetes cluster, and this process is automated using a CI/CD pipeline. The annotation component (fourth box of the bottom row) is discussed in the next section.</p>
<p>[^quarto]Successor to R Markdown, Quarto has become an essential tool of our data stack. It unifies the functionality of several very useful packages from the R Markdown ecosystem while providing native support of several programming languages, including Python and Julia in addition to R. It is increasingly used at Insee as a way to produce reproducible documents and output them in a variety of formats.</p>
<div id="fig-full-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-full-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/annotation-datalab.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-full-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Our implementation of a complete MLOps architecture
</figcaption>
</figure>
</div>
</section>
<section id="sec-annotation" class="level3" data-number="4.3.5">
<h3 data-number="4.3.5" class="anchored" data-anchor-id="sec-annotation"><span class="header-section-number">4.3.5</span> Promoting continuous improvement of the model</h3>
<p>The monitoring layer of our application provides an important and detailed view into the system performance. Due to the dynamic nature of real-life data, ML models’ performance often declines over time. To promote continuous improvement of the model, it is thus essential to envision strategies to overcome these performance losses. A frequently used strategy is periodic re-training of the model, requiring the collection of new training data.</p>
<p>Several months after the first version of the model was deployed in production, the need to implement a continuous annotation process became increasingly apparent for two key reasons. First, a gold-standard sample was not accessible at the time of the experimentation phase, so we relied on a subset of the training dataset to perform evaluation, knowing the labeling quality was not optimal. Continuously collecting a gold-standard sample would thus enable us to get an unbiased view of the model’s performance in production on real data, particularly on data that has been automatically coded. Another reason is the redesign of the NACE statistical classification in 2025. From 2025 onwards, NSOs will be required to use the latest version of NACE, namely NACE Rev.&nbsp;2.1. This revision brings substantial changes that will require an adaptation of the model, and thus the collection of new training set. Annotation of the old training dataset according to the new statistical classification will be necessary.</p>
<p>Against that background, an annotation campaign has been initiated in early 2024 to continuously build a gold-standard dataset. The annotation campaign is carried out on the SSP Cloud using Label Studio, an open-source annotation tool that provides a user-friendly interface and is available in Onyxia’s catalog. <a href="#fig-full-architecture" class="quarto-xref">Figure&nbsp;6</a> illustrates how the labelling component (fourth box of the bottom row) could be readily integrated in the project architecture thanks to its modular nature. In practice, we create a pool of text descriptions randomly sampled from the data passed through the API over the past three months. This sample is then sent to annotation by NACE experts using the UI of Label Studio. The annotation results are automatically saved on MinIO, transformed into Parquet format. Then, these gold-standard data are directly integrated into the monitoring dashboard to compute and observe various model performance metrics. These metrics give us a much clearer picture on the actual performance of the model on production data, and in particular on its shortcomings. In parallel, we will launch an annotation campaign in the upcoming months to construct a new training set tailored to the recently updated NACE statistical classification. Leveraging both the updated training data and performance metrics derived from the gold-standard sample, we aim to iteratively enhance the model’s accuracy through periodical and targeted re-training in the near future.</p>
<!-- ############################################################################################################## -->
<!-- ############################################################################################################## -->
<!-- ############################################################################################################## -->



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-joulin2016bag" class="csl-entry" role="listitem">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. <span>“Bag of Tricks for Efficient Text Classification.”</span> <em>arXiv Preprint arXiv:1607.01759</em>.
</div>
<div id="ref-li2022survey" class="csl-entry" role="listitem">
Li, Qian, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S Yu, and Lifang He. 2022. <span>“A Survey on Text Classification: From Traditional to Deep Learning.”</span> <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 13 (2): 1–41.
</div>
<div id="ref-meyer_sicore_1997" class="csl-entry" role="listitem">
Meyer, Eric, and Pascal Rivière. 1997. <span>“<span>SICORE</span>, Un Outil Et Une Méthode Pour Le Chiffrement Automatique à <span>I</span>’<span>INSEE</span>.”</span> In <em>Actes de La 4ème <span>Conférence</span> <span>Internationale</span> Des <span>Utilisateurs</span> de <span>Blaise</span></em>, 280–93. Paris, France. <a href="http://www.blaiseusers.org/1997/papers/meyer97.pdf">http://www.blaiseusers.org/1997/papers/meyer97.pdf</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/thomasfaria\.github\.io\/retex-innovation-insee");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../src/implementation/index.html" class="pagination-link" aria-label="3 - Implementations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Implementations</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../src/discussion/index.html" class="pagination-link" aria-label="5 - Discussion">
        <span class="nav-page-text">5 - Discussion</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>