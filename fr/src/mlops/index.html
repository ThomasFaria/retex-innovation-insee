<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>index – SSP Cloud Datalab</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../src/discussion/index.html" rel="next">
<link href="../../src/implementation/index.html" rel="prev">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6f142b21ac06632926f739cb167951b6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../src/mlops/index.html">4 - MLOps</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">SSP Cloud Datalab</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-translate"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../../fr/">
            Français
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../../en/">
            English
            </a>
          </li>
      </ul>
    </div>
    <a href="../../src/pdf-fr/index.pdf" title="PDF" class="quarto-navigation-tool px-1" aria-label="PDF"><i class="bi bi-file-pdf-fill"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/introduction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/principles/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Principes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/implementation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Implémentation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/mlops/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4 - MLOps</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../src/discussion/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Discussion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-mlops-fr" id="toc-sec-mlops-fr" class="nav-link active" data-scroll-target="#sec-mlops-fr"><span class="header-section-number">4</span> Cas d’usage : adopter les pratiques MLOps pour améliorer la codification de l’APE</a>
  <ul class="collapse">
  <li><a href="#fluidifier-la-codification-de-lape-à-laide-de-méthodes-dapprentissage-automatique" id="toc-fluidifier-la-codification-de-lape-à-laide-de-méthodes-dapprentissage-automatique" class="nav-link" data-scroll-target="#fluidifier-la-codification-de-lape-à-laide-de-méthodes-dapprentissage-automatique"><span class="header-section-number">4.1</span> Fluidifier la codification de l’APE à l’aide de méthodes d’apprentissage automatique</a>
  <ul class="collapse">
  <li><a href="#motivation-1" id="toc-motivation-1" class="nav-link" data-scroll-target="#motivation-1"><span class="header-section-number">4.1.1</span> Motivation</a></li>
  <li><a href="#la-tâche-de-codification" id="toc-la-tâche-de-codification" class="nav-link" data-scroll-target="#la-tâche-de-codification"><span class="header-section-number">4.1.2</span> La tâche de codification</a></li>
  <li><a href="#méthodologie" id="toc-méthodologie" class="nav-link" data-scroll-target="#méthodologie"><span class="header-section-number">4.1.3</span> Méthodologie</a></li>
  </ul></li>
  <li><a href="#une-approche-orientée-production-et-mlops" id="toc-une-approche-orientée-production-et-mlops" class="nav-link" data-scroll-target="#une-approche-orientée-production-et-mlops"><span class="header-section-number">4.2</span> Une approche orientée production et MLOps</a>
  <ul class="collapse">
  <li><a href="#sec-devops-mlops" id="toc-sec-devops-mlops" class="nav-link" data-scroll-target="#sec-devops-mlops"><span class="header-section-number">4.2.1</span> Du DevOps au MLOps</a></li>
  <li><a href="#sec-principles-mlops" id="toc-sec-principles-mlops" class="nav-link" data-scroll-target="#sec-principles-mlops"><span class="header-section-number">4.2.2</span> Principes du MLOps</a></li>
  <li><a href="#implémentation-avec-mlflow" id="toc-implémentation-avec-mlflow" class="nav-link" data-scroll-target="#implémentation-avec-mlflow"><span class="header-section-number">4.2.3</span> Implémentation avec MLflow</a></li>
  </ul></li>
  <li><a href="#faciliter-le-développement-itératif-avec-les-technologies-cloud" id="toc-faciliter-le-développement-itératif-avec-les-technologies-cloud" class="nav-link" data-scroll-target="#faciliter-le-développement-itératif-avec-les-technologies-cloud"><span class="header-section-number">4.3</span> Faciliter le développement itératif avec les technologies cloud</a>
  <ul class="collapse">
  <li><a href="#un-environnement-de-développement-flexible" id="toc-un-environnement-de-développement-flexible" class="nav-link" data-scroll-target="#un-environnement-de-développement-flexible"><span class="header-section-number">4.3.1</span> Un environnement de développement flexible</a></li>
  <li><a href="#déploiement-dun-modèle" id="toc-déploiement-dun-modèle" class="nav-link" data-scroll-target="#déploiement-dun-modèle"><span class="header-section-number">4.3.2</span> Déploiement d’un modèle</a></li>
  <li><a href="#construction-dune-pipeline-intégrée" id="toc-construction-dune-pipeline-intégrée" class="nav-link" data-scroll-target="#construction-dune-pipeline-intégrée"><span class="header-section-number">4.3.3</span> Construction d’une pipeline intégrée</a></li>
  <li><a href="#sec-monitoring" id="toc-sec-monitoring" class="nav-link" data-scroll-target="#sec-monitoring"><span class="header-section-number">4.3.4</span> Surveillance d’un modèle en production</a></li>
  <li><a href="#définition-du-seuil-de-codification-automatique-et-surveillance-en-production" id="toc-définition-du-seuil-de-codification-automatique-et-surveillance-en-production" class="nav-link" data-scroll-target="#définition-du-seuil-de-codification-automatique-et-surveillance-en-production"><span class="header-section-number">4.3.5</span> Définition du seuil de codification automatique et surveillance en production</a></li>
  <li><a href="#sec-annotation" id="toc-sec-annotation" class="nav-link" data-scroll-target="#sec-annotation"><span class="header-section-number">4.3.6</span> Favoriser l’amélioration continue du modèle</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<!-- ############################################################################################################## -->
<!-- ############################################################################################################## -->
<!-- ############################################################################################################## -->
<section id="sec-mlops-fr" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Cas d’usage : adopter les pratiques MLOps pour améliorer la codification de l’APE</h1>
<p>Ce chapitre vise à illustrer comment l’Insee a réussi à déployer son premier modèle de machine learning (ML) en production. Il propose une description détaillée de l’approche MLOps à laquelle ce projet s’est efforcé d’adhérer, en mettant l’accent sur les différentes technologies employées. En particulier, nous soulignons le rôle crucial des technologies <em>cloud</em> qui ont permis la construction du projet de manière itérative, ainsi que la manière dont Onyxia a grandement facilité cette construction en fournissant des environnements de développement flexibles et des outils pour entraîner, déployer et surveiller les modèles des modèles d’apprentissage automatique. De plus, la convergence entamée des environnement de self (^<span class="math inline">\(LS^3\)</span>), de développement (KubeDev) et de production (KubeProd) constitue une réelle avancer pour faciliter la mise en production d’autres modèles d’apprentissage automatique (un modèle de codification de la PCS a également été déployé récemment). Le projet présenté est totalement disponible en open source <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> et reste en cours de développement actif.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<a href="">https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories</a></p></div></div><section id="fluidifier-la-codification-de-lape-à-laide-de-méthodes-dapprentissage-automatique" class="level2 page-columns page-full" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="fluidifier-la-codification-de-lape-à-laide-de-méthodes-dapprentissage-automatique"><span class="header-section-number">4.1</span> Fluidifier la codification de l’APE à l’aide de méthodes d’apprentissage automatique</h2>
<section id="motivation-1" class="level3 page-columns page-full" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="motivation-1"><span class="header-section-number">4.1.1</span> Motivation</h3>
<p>Les tâches de codification sont des opérations bien connues des instituts statistiques, et peuvent parfois être complexes en raison de la taille des nomenclatures. À l’Insee, un outil sophistiqué appelé Sicore a été développé dans les années 1990 pour effectuer diverses tâches de classification <span class="citation" data-cites="meyer_sicore_1997">(<a href="#ref-meyer_sicore_1997" role="doc-biblioref">Meyer and Rivière 1997</a>)</span>. Cet outil repose sur un ensemble de règles déterministes permettant d’identifier les codes corrects à partir d’un libellé textuel en se basant sur un fichier de référence comprenant un certain nombre d’exemples. Chaque libellé d’entrée est soumis à ces règles et, lorsqu’un code correct est reconnu, il est attribué au libellé. En revanche, si le libellé n’est pas reconnu, il doit être classer manuellement par un agent de l’Insee.</p>
<p>Deux raisons principales ont motivé l’expérimentation de nouvelles méthodes de codification.</p>
<p>Premièrement, un changement interne est survenu avec la refonte du répertoire statistique des entreprises en France (Sirene), qui liste toutes les entreprises et leur attribue un identifiant unique utilisé par les administrations publiques, le numéro Siren. Les principaux objectifs de cette refonte étaient d’améliorer la gestion quotidienne du répertoire pour les agents de l’Insee et de réduire les délais d’attente pour les entreprises. Par ailleurs, au niveau national, le gouvernement a lancé, dans le cadre de la loi PACTE (n° 2019-486 du 22 mai 2019), un guichet unique pour les formalités des entreprises, offrant aux chefs d’entreprises plus de flexibilité dans la description de leurs activités principales les rendant ainsi plus verbeux que précédemment. Les tests initiaux ont révélé que Sicore n’était plus adapté pour effectuer la codification APE, puisque seulement <span class="math inline">\(30\%\)</span> des liasses d’entreprises étaient automatiquement codées, et donc <span class="math inline">\(70\%\)</span> devait être codés manuellement par des gestionnaires. Les équipes en charge du répertoire Sirene, déjà confrontées à des charges de travail importantes et à de fortes contraintes opérationnelles, ne pouvaient pas voir leur charge augmentée par une re-codification manuelle, une tâche à la fois chronophage et peu stimulante. Ainsi, en mai 2022, la décision a été prise d’expérimenter de nouvelles méthodes pour effectuer cette tâche de codification, avec pour objectif de les utiliser en production dès le 1er janvier 2023, date de lancement du nouveau répertoire Sirene.</p>
<p>Trois parties prenantes étaient donc impliquées dans ce projet : l’équipe métier (division RIAS<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>), responsable de la gestion du répertoire statistique des entreprises ; l’équipe informatique, en charge du développement des applications liés au fonctionnement du répertoire ; et l’équipe d’innovation (l’unité SSP Lab), responsable de la mise en œuvre du nouvel outil de codification.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Répertoire Interadministratif Sirene</p></div></div></section>
<section id="la-tâche-de-codification" class="level3 page-columns page-full" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="la-tâche-de-codification"><span class="header-section-number">4.1.2</span> La tâche de codification</h3>
<p>Le projet présenté consiste en un problème classique de classification dans le cadre de traitement de langage naturel. À partir d’une description textuelle de l’activité d’une entreprise, l’objectif est de prédire la classe associée dans la nomenclature APE. Cette classification présente la particularité d’être hiérarchique et comporte cinq niveaux différents<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> : section, division, groupe, catégorie et sous-catégorie. Au total, la nomenclature comprend 732 sous-classes, ce qui correspond au niveau le plus fin de la nomenclature et pour lequel on souhaite réaliser notre codification. La table <a href="#tbl-nace-nomenclature" class="quarto-xref">Table&nbsp;1</a> fournit un exemple de cette structure hiérarchique.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;En réalité, il existe cinq niveaux en France, mais seulement quatre au niveau européen.</p></div></div><div id="tbl-nace-nomenclature" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nace-nomenclature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Nomenclature APE
</figcaption>
<div aria-describedby="tbl-nace-nomenclature-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 16%">
<col style="width: 61%">
<col style="width: 2%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Niveau</strong></th>
<th><strong>NACE</strong></th>
<th><strong>Title</strong></th>
<th><strong>Size</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Section</td>
<td>H</td>
<td>Transports et entreposage</td>
<td>21</td>
</tr>
<tr class="even">
<td>Division</td>
<td>52</td>
<td>Entreposage et services auxiliaires des transports</td>
<td>88</td>
</tr>
<tr class="odd">
<td>Groupe</td>
<td>522</td>
<td>Services auxiliaires des transports</td>
<td>272</td>
</tr>
<tr class="even">
<td>Catégorie</td>
<td>5224</td>
<td>Manutention</td>
<td>615</td>
</tr>
<tr class="odd">
<td><strong>Sous-catégorie</strong></td>
<td><span class="red2"><strong>5224A</strong></span></td>
<td><strong>Manutention portuaire</strong></td>
<td><span class="orange"><strong>732</strong></span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Avec la mise en place du guichet unique, les chefs d’entreprise décrivent désormais leur activité dans un champ de texte libre. Par conséquent, les nouveaux libellés diffèrent fortement des libellés harmonisés précédemment reçus. Il a donc été décidé de travailler avec des modèles d’apprentissage automatique, reconnus pour leur efficacité sur les tâches de classification supervisée de texte <span class="citation" data-cites="li2022survey">(<a href="#ref-li2022survey" role="doc-biblioref">Li et al. 2022</a>)</span>. Cela représente un changement de paradigme significatif pour l’Insee, puisque le <em>machine learning</em> n’est traditionnellement pas utilisé dans la production des statistiques officielles. De plus, la perspective de mettre le nouveau modèle en production a été envisagée dès le début du projet, orientant de nombreux choix méthodologiques et techniques. Ainsi, plusieurs décisions stratégiques ont dû être rapidement prises, notamment en ce qui concerne la méthodologie, le choix d’un environnement de développement cohérent avec l’environnement de production cible, et l’adoption de méthodes de travail collaboratif.</p>
</section>
<section id="méthodologie" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="méthodologie"><span class="header-section-number">4.1.3</span> Méthodologie</h3>
<p>La classification textuelle à partir des champs de texte libre fournis par les chefs d’entreprise est une tâche complexe : les descriptions d’activité sont relativement courtes et contiennent donc peu d’information statistique, peuvent inclure des fautes d’orthographe et nécessitent souvent une expertise métier pour être correctement codées. Pour une telle tâche, les méthodes traditionnelles d’analyse textuelle, comme la vectorisation par comptage ou TF-IDF, sont souvent insuffisantes, tandis que les méthodes d’intégration basées sur des réseaux de neurones tendent à donner de meilleurs résultats <span class="citation" data-cites="li2022survey">(<a href="#ref-li2022survey" role="doc-biblioref">Li et al. 2022</a>)</span>. Cependant, ces architectures nécessitent souvent des ressources de calcul importantes, et peuvent exiger du matériel spécifique, comme des GPUs, afin obtenir une latence acceptable lors de l’inférence. Ces contraintes nous ont, dans un premier temps, éloignés des modèles les plus performants, tels que les modèles Transformer, et orientés vers le modèle fastText <span class="citation" data-cites="joulin2016bag">(<a href="#ref-joulin2016bag" role="doc-biblioref">Joulin et al. 2016</a>)</span>, un réseau de neurone plus simple basé sur des plongements lexicaux. Le modèle fastText est extrêmement rapide à entraîner, et l’inférence ne nécessite pas de GPU pour obtenir un temps de latence faible. En outre, le modèle a donné d’excellents résultats pour notre cas d’usage, qui, compte tenu des contraintes de temps et de ressources humaines, étaient largement suffisants pour améliorer le processus existant. Enfin, l’architecture du modèle est relativement simple, ce qui facilite la communication et l’adoption au sein des différentes équipes de l’Insee.</p>
<p>Le modèle fastText repose sur une approche de sac de mots (<em>bag-of-words</em>) pour obtenir des plongements lexicaux et une couche de classification basée sur la régression logistique. L’approche sac de mots consiste à représenter un texte comme un ensemble de représentations vectorielles de chacun des mots qui le composent. La spécificité du modèle fastText, par rapport à d’autres approches basées sur des plongements lexicaux, est que les plongements lexicaux ne sont pas seulement calculés sur les mots, mais aussi sur des <em>n-grams</em> de mots et de caractères, fournissant ainsi plus de contexte et réduisant les biais liés aux fautes d’orthographe. Ensuite, le plongement lexical d’une phrase est calculé comme une fonction des plongements lexicaux des mots (et n-grams de mots et de caractères), généralement une moyenne. Dans le cas de la classification textuelle supervisée, la matrice de plongement et les poids du <em>classifier</em> sont appris simultanément lors de l’entraînement par descente de gradient, en minimisant la fonction de perte d’entropie croisée.</p>
<p><a href="#fig-fasttext" class="quarto-xref">Figure&nbsp;1</a> présente la pipeline complete des opérations effectuées par fastText sur un exemple de texte en entrée.</p>
<div id="fig-fasttext" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/fasttext.png" class="img-fluid figure-img" style="width:125.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Aperçu simplifié du processus derrière les classifications fastText
</figcaption>
</figure>
</div>
</section>
</section>
<section id="une-approche-orientée-production-et-mlops" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="une-approche-orientée-production-et-mlops"><span class="header-section-number">4.2</span> Une approche orientée production et MLOps</h2>
<p>Dès le début du projet, l’objectif était d’aller au-delà de la simple expérimentation et de mettre le modèle en production. Par ailleurs, ce projet pilote avait également pour but de servir de modèle pour les futurs projets de <em>machine learning</em> à l’Insee. Nous avons donc cherché à appliquer les meilleures pratiques de développement dès les premières étapes du projet : respect des standards de qualité de code de la communauté, utilisation de scripts pour le développement au lieu de notebooks, construction d’une structure modulaire semblable à un <em>package</em>, etc. Cependant, par rapport aux projets de développement traditionnels, les projets de <em>machine learning</em> présentent des caractéristiques spécifiques qui nécessitent l’application d’un ensemble de bonnes pratiques complémentaire, regroupées sous le nom de <strong>MLOps</strong>.</p>
<section id="sec-devops-mlops" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-devops-mlops"><span class="header-section-number">4.2.1</span> Du DevOps au MLOps</h3>
<p>Le <strong>DevOps</strong> est un ensemble de pratiques conçu pour favoriser la collaboration entre les équipes de développement (<em>Dev</em>) et d’opérations (<em>Ops</em>). L’idée fondamentale est d’intégrer tout le cycle de vie d’un projet dans un continuum automatisé. Un outil important pour atteindre cette continuité sont les pipelines CI/CD. Avec l’intégration continue (<em>Continuous Integration</em> ou CI), chaque <em>commit</em> de nouveau code source déclenche un processus d’opérations standardisées, telles que la construction de l’application, son test et sa mise à disposition sous forme de version. Ensuite, le déploiement continu (<em>Continuous Deployment</em> ou CD) consiste en des outils pour automatiser le déploiement du nouveau code et limiter les interventions manuelles, tout en garantissant une supervision appropriée pour assurer la stabilité et la sécurité des processus. Cette approche favorise un déploiement plus rapide et continu des modifications ou ajouts nécessaires de fonctionnalités. En outre, en encourageant la collaboration entre les équipes, le DevOps accélère également le cycle d’innovation, permettant aux équipes de résoudre les problèmes au fur et à mesure qu’ils surviennent et d’intégrer efficacement les retours tout au long du cycle de vie du projet.</p>
<p>L’approche <strong>MLOps</strong> peut être vue comme une extension du DevOps, développée pour relever les défis spécifiques liés à la gestion du cycle de vie des modèles de <em>machine learning</em> (ML). Fondamentalement, DevOps et MLOps partagent le même objectif : construire des logiciels de manière plus automatisée et robuste. La principale différence réside dans le fait qu’avec le MLOps, le logiciel inclut également une composante de machine learning. Par conséquent, le cycle de vie du projet devient plus complexe. Le modèle de ML sous-jacent doit être réentraîné régulièrement afin d’éviter toute perte de performance au fil du temps. L’ingestion des données doit également être intégrée au processus, car de nouvelles données peuvent être utilisées pour améliorer les performances. <a href="#fig-mlops-cycle" class="quarto-xref">Figure&nbsp;2</a> présente les étapes d’un projet de ML en utilisant une représentation continue, comme cela se fait traditionnellement en DevOps. Cela illustre un principe fondamental du MLOps : la nécessité d’une amélioration continue, décrite plus en détail dans <a href="#sec-principles-mlops" class="quarto-xref">Section&nbsp;4.2.2</a>.</p>
<div id="fig-mlops-cycle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlops-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/mlops-cycle.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlops-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: L’approche MLOps favorise une gestion continue du cycle de vie des projets de ML
</figcaption>
</figure>
</div>
</section>
<section id="sec-principles-mlops" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-principles-mlops"><span class="header-section-number">4.2.2</span> Principes du MLOps</h3>
<p>Le MLOps repose sur quelques principes fondamentaux qui sont essentiels pour construire des applications de <em>machine learning</em> évolutives et prêtes pour le passage en production. Ces principes visent à relever les défis spécifiques associés aux chaînes de production de <em>machine learning</em>.</p>
<p>Le principe le plus fondamental du MLOps est l’amélioration continue, reflétant la nature itérative des projets de ML. Lors de la phase d’expérimentation, le modèle est développé à partir d’un ensemble de données d’entraînement, qui diffère généralement des données de production à certains égards. Une fois le modèle déployé en production, les nouvelles données sur lesquelles le modèle doit effectuer des prédictions peuvent révéler des informations sur ses performances et ses éventuelles lacunes. Ces informations nécessitent un retour à la phase d’expérimentation, où les data scientists ajustent ou redéfinissent leurs modèles pour corriger les problèmes découverts ou améliorer la précision. Ce principe souligne donc l’importance de construire une boucle de rétroaction permettant des améliorations continues tout au long du cycle de vie d’un modèle. L’automatisation, en particulier grâce à l’utilisation de pipelines CI/CD, joue un rôle crucial en rendant la transition entre les phases d’expérimentation et de production plus fluide. La surveillance (<em>monitoring</em>) est également une composante essentielle de ce processus : un modèle déployé en production doit être continuellement analysé pour détecter d’éventuelles dérives importantes susceptibles de réduire ses performances prédictives et nécessitant des ajustements supplémentaires, comme un ré-entraînement.</p>
<p>Un autre objectif majeur du MLOps est de promouvoir la reproductibilité, en garantissant que toute expérience de ML puisse être reproduite de manière fiable avec les mêmes résultats. Les outils de MLOps facilitent ainsi une sauvegarde détaillée des expériences de ML, incluant les étapes de prétraitement des données, les hyperparamètres des modèles utilisés et les algorithmes d’entraînement. Les données, modèles et codes sont versionnés, permettant aux équipes de revenir à des versions antérieures si une mise à jour ne donne pas les résultats escomptés. Enfin, ces outils aident à produire des spécifications détaillées de l’environnement informatique utilisé pour produire ces expériences — comme les versions des bibliothèques — et reposent souvent sur des conteneurs pour reproduire les mêmes conditions que celles dans lesquelles le modèle initial a été développé.</p>
<p>Enfin, le MLOps vise à favoriser le travail collaboratif. Les projets basés sur le ML impliquent généralement une gamme plus large de profils : équipes métier et équipes de <em>data science</em> d’un côté, développeurs et équipes de production informatique de l’autre. Comme le DevOps, le MLOps met donc l’accent sur la nécessité d’une culture collaborative et d’éviter le travail en silos. Les outils de MLOps incluent généralement des fonctionnalités collaboratives, telles que des stockages centralisés pour les modèles de ML ou les caractéristiques (<em>features</em>) de ML, qui facilitent le partage des composants entre les membres des équipes et limitent la redondance.</p>
</section>
<section id="implémentation-avec-mlflow" class="level3 page-columns page-full" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="implémentation-avec-mlflow"><span class="header-section-number">4.2.3</span> Implémentation avec MLflow</h3>
<p>De nombreux outils ont été développés pour mettre en œuvre l’approche MLOps dans des projets concrets. Tous visent à appliquer, sous une forme ou une autre, les principes fondamentaux décrits précédemment. Dans ce projet, nous avons choisi de nous appuyer sur un outil open-source populaire nommé <strong>MLflow</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Ce choix ne reflète pas une supériorité inhérente de MLflow par rapport à d’autres outil, mais s’explique par un ensemble de bonnes propriétés associées à MLflow, qui en font une solution particulièrement pertinente pour notre cas d’usage. Tout d’abord, il couvre l’intégralité du cycle de vie des projets de ML, tandis que d’autres outils peuvent être plus spécialisés sur certaines parties seulement. Ensuite, il offre une grande interopérabilité grâce à une bonne interface avec les bibliothèques populaires de ML — telles que PyTorch, Scikit-learn, XGBoost, etc. — et prend en charge plusieurs langages de programmation — notamment Python, R et Java, couvrant ainsi le spectre des langages couramment utilisés à l’Insee. Enfin, MLflow s’est révélé très facile d’utilisation, encourageant ainsi son adoption par les membres du projet et facilitant la collaboration continue entre eux.</p>
<!-- TODO: est ce qu'on rajoute que c'est l'outil choisi à l'Insee pour tous les projets de ML ? -->
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<a href="">https://github.com/MLflow/MLflow</a></p></div></div><p>MLflow fournit un cadre cohérent pour opérationnaliser les principes du MLOps efficacement au sein des projets de ML. Les data scientists peuvent encapsuler leur travail dans des <em>MLflow Projects</em> qui regroupent le code ML et ses dépendances, garantissant que chaque projet soit reproductible et puisse être ré-exécuté de manière identique. Un projet s’appuie sur un <em>MLflow Model</em>, un format standardisé compatible avec la plupart des bibliothèques de ML et offrant une méthode normalisée pour déployer le modèle, par exemple via une API. Cette interopérabilité et cette standardisation sont essentielles pour soutenir l’amélioration continue du projet, puisque les modèles entraînés avec une multitude de packages peuvent être facilement comparés ou remplacés les uns par les autres sans casser le code existant.</p>
<p>À mesure que les expériences avec différents modèles progressent, le <em>Tracking Server</em> enregistre des informations détaillées sur chaque exécution — hyperparamètres, métriques, artefacts et données — ce qui favorise à la fois la reproductibilité et facilite la phase de sélection des modèles grâce à une interface utilisateur ergonomique. Une fois la phase d’expérimentation terminée, les modèles sélectionnés sont rajoutés dans le <em>Model Registry</em>, où ils sont versionnés et prêts pour le déploiement. Cet entrepôt sert de “magasin” centralisé pour les modèles, permettant aux différents membres ou équipes du projet de gérer collaborativement le cycle de vie du projet.</p>
<p><a href="#fig-mlflow-components" class="quarto-xref">Figure&nbsp;3</a> illustre les composants principaux de MLflow et la manière dont ils facilitent un flux de travail plus continu et collaboratif au sein d’un projet de ML.</p>
<div id="fig-mlflow-components" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlflow-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/mlflow-model-registry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlflow-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Composants principaux de MLflow. Source : Databricks.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="faciliter-le-développement-itératif-avec-les-technologies-cloud" class="level2 page-columns page-full" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="faciliter-le-développement-itératif-avec-les-technologies-cloud"><span class="header-section-number">4.3</span> Faciliter le développement itératif avec les technologies cloud</h2>
<p>Bien que l’amélioration continue soit un principe fondamental du MLOps, elle est également très exigeante. En particulier, elle nécessite de concevoir et de construire un projet sous la forme d’une <em>pipeline</em> intégrée, dont les différentes étapes sont principalement automatisées, de l’ingestion des données jusqu’à la surveillance du modèle en production. Dans ce contexte, le développement itératif est essentiel pour construire un produit minimum viable qui sera ensuite affiné et amélioré au fil du temps. Cette section illustre comment les technologies <em>cloud</em>, via le projet Onyxia, ont été déterminantes pour construire le projet sous forme de composants modulaires interconnectés, renforçant ainsi considérablement la capacité de raffinement continu au fil du temps.</p>
<section id="un-environnement-de-développement-flexible" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="un-environnement-de-développement-flexible"><span class="header-section-number">4.3.1</span> Un environnement de développement flexible</h3>
<p>Dans un projet de ML, la flexibilité de l’environnement de développement est essentielle. Premièrement, en raison de la diversité des tâches à accomplir : collecte des données, prétraitement, modélisation, évaluation, inférence, surveillance, etc. Deuxièmement, parce que le domaine du ML évolue rapidement, il est préférable de construire une application de ML sous forme d’un ensemble de composants modulaires afin de pouvoir mettre à jour certains éléments sans perturber l’ensemble de la pipeline. Comme discuté dans la <a href="../../src/principles/index.html#sec-cloud-native">Section 2.2</a>, les technologies <em>cloud</em> permettent de créer des environnements de développement modulaires et évolutifs.</p>
<p>Cependant, comme également abordé dans la <a href="../../src/implementation/index.html#sec-implementation">Section 3</a>, l’accès à ces ressources ne suffit pas. Un projet de ML nécessite une grande variété d’outils pour se conformer aux principes du MLOps : stockage des données, environnements de développement interactifs pour expérimenter librement, outils d’automatisation, outils de surveillance, etc. Bien que ces outils puissent être installés sur un cluster Kubernetes, il est essentiel de les rendre disponibles aux data scientists de manière intégrée et préconfigurée pour faciliter leur adoption. Grâce à son catalogue de services et à l’injection automatique de configurations dans les services, Onyxia permet de construire des projets qui reposent sur plusieurs composants <em>cloud</em> capables de communiquer facilement entre eux.</p>
<p>La manière dont l’entraînement du modèle a été réalisé pour ce projet illustre bien la flexibilité offerte par Onyxia pendant la phase d’expérimentation. Tout le code utilisé pour l’entraînement est écrit en Python au sein d’un service VSCode. Grâce à l’injection automatique des identifiants personnels S3 dans chaque service au démarrage, les différents utilisateurs du projet peuvent interagir directement avec les données d’entraînement stockées dans un <em>bucket</em> S3 sur MinIO, la solution de stockage d’objets par défaut d’Onyxia. Toutes les expériences menées lors de la phase de sélection du modèle sont consignées dans une instance partagée de MLflow, qui enregistre les données sur une instance PostgreSQL automatiquement lancée sur Kubernetes, tandis que les artefacts (modèles entraînés et métadonnées associées) sont stockés sur MinIO.</p>
<p>Le modèle a été entraîné en utilisant une recherche exhaustive (<em>grid-search</em>) pour l’ajustement des hyperparamètres et évalué par validation croisée (<em>cross-validation</em>). Cette combinaison, reconnue pour offrir une meilleure évaluation des performances de généralisation du modèle, nécessite cependant d’importantes ressources de calcul en raison de la nature combinatoire du test de nombreuses combinaisons d’hyperparamètres. Dans notre cas, nous avons tiré parti d’Argo Workflows, un moteur de <em>workflows</em> open source conçu pour orchestrer des tâches parallèles sur Kubernetes, chaque tâche étant spécifiée comme un conteneur indépendant. Cela a permis de comparer facilement les performances des différents modèles entraînés et de sélectionner le meilleur en utilisant les outils de comparaison et de visualisation disponibles dans l’interface utilisateur de MLflow.</p>
<p>En résumé, la phase d’entraînement a été rendue à la fois efficace et reproductible grâce à l’utilisation de nombreux composants modulaires interconnectés — une caractéristique distinctive des technologies <em>cloud</em> — mis à disposition des <em>data scientists</em> grâce à Onyxia.</p>
</section>
<section id="déploiement-dun-modèle" class="level3 page-columns page-full" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="déploiement-dun-modèle"><span class="header-section-number">4.3.2</span> Déploiement d’un modèle</h3>
<p>Une fois que les modèles candidats ont été optimisés, évalués et qu’un modèle performant a été sélectionné, l’étape suivante consiste à le rendre accessible aux utilisateurs finaux de l’application. Fournir simplement le modèle entraîné sous forme d’artefact, ou même uniquement le code pour l’entraîner, n’est pas une manière optimale de le transmettre, car cela suppose que les utilisateurs disposent des ressources, de l’infrastructure et des connaissances nécessaires pour l’entraîner dans les mêmes conditions. L’objectif est donc de rendre le modèle accessible de manière simple et interopérable, c’est-à-dire qu’il doit être possible de l’interroger avec divers langages de programmation et par d’autres applications de manière programmatique.</p>
<p>Dans ce contexte, nous avons choisi de déployer le modèle via une API REST. Cette technologie est devenue une norme pour servir des modèles de ML, car elle présente plusieurs avantages. Tout d’abord, elle s’intègre parfaitement dans un environnement orienté <em>cloud</em> : comme les autres composants de notre stack, elle permet d’interroger le modèle en utilisant des requêtes HTTP standard, ce qui contribue à la modularité du système. De plus, elle est interopérable : reposant sur des technologies standards pour les requêtes (requêtes HTTP) et les réponses (généralement une chaîne formatée en JSON), elle est largement indépendante du langage de programmation utilisé pour effectuer les requêtes. Enfin, les API REST offrent une grande évolutivité grâce à leur conception sans état (<em>stateless</em>)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Chaque requête contient toutes les informations nécessaires pour être comprise et traitée, ce qui permet de dupliquer facilement l’API sur différentes machines pour répartir une charge importante — un processus connu sous le nom de scalabilité horizontale.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;La conception sans état (<em>stateless</em>) fait référence à une architecture système où chaque requête d’un client au serveur contient toutes les informations nécessaires pour comprendre et traiter la requête. Cela signifie que le serveur ne stocke aucune information sur l’état du client entre les requêtes, ce qui permet de traiter chaque requête indépendamment. Cette conception simplifie l’évolutivité et renforce la robustesse du système, car n’importe quel serveur peut gérer une requête sans dépendre des interactions précédentes.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<a href="">https://fastapi.tiangolo.com</a></p></div></div><p>Nous avons développé l’API servant le modèle avec FastAPI<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, un framework web rapide et bien documenté pour construire des APIs avec Python. Le code de l’API et les dépendances logicielles nécessaires sont encapsulés dans une image Docker, ce qui permet de la déployer sous forme de conteneur sur le cluster Kubernetes. L’un des avantages majeurs de Kubernetes est sa capacité d’adapter la puissance de l’API — via le nombre de pods d’API effectivement déployés — en fonction de la demande, tout en fournissant un équilibrage de charge automatique. Au démarrage, l’API récupère automatiquement le modèle approprié depuis l’entrepôt de modèles MLflow stocké sur MinIO. Enfin, comme le code de l’application est <em>packagé</em> en utilisant l’API standardisée de MLflow — permettant par exemple d’intégrer directement l’étape de prétraitement dans chaque appel API — le code d’inférence reste largement uniforme, quel que soit le framework de ML sous-jacent utilisé. Ce processus de déploiement est résumé dans <a href="#fig-api-datalab" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div id="fig-api-datalab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-api-datalab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/api-datalab.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-api-datalab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Une approche basé sur des technologies cloud pour servir un modèle de ML via une API REST
</figcaption>
</figure>
</div>
</section>
<section id="construction-dune-pipeline-intégrée" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="construction-dune-pipeline-intégrée"><span class="header-section-number">4.3.3</span> Construction d’une pipeline intégrée</h3>
<p>L’architecture construite à ce stade reflète déjà certains principes importants du MLOps. L’utilisation de la conteneurisation pour déployer l’API, ainsi que celle de MLflow pour suivre les expérimentations pendant le développement du modèle, garantit la reproductibilité des prédictions. L’utilisation de l’entrepôt central de modèles fourni par MLflow facilite la gestion du cycle de vie des modèles de manière collaborative. De plus, la modularité de l’architecture laisse de la place pour des améliorations ultérieures, puisque des composants modulaires peuvent être ajoutés ou modifiés facilement sans casser la structure du projet dans son ensemble. Comme nous le verrons dans les sections suivantes, cette propriété s’est avérée essentielle pour construire le projet de manière itérative, permettant d’ajouter une couche de surveillance du modèle (<a href="#sec-monitoring" class="quarto-xref">Section&nbsp;4.3.4</a>) et un composant d’annotation (<a href="#sec-annotation" class="quarto-xref">Section&nbsp;4.3.6</a>) afin de favoriser l’amélioration continue du modèle en intégrant “l’humain dans le cycle de vie du modèle de ML” (<em>human in the loop</em>).</p>
<p>Cependant, la capacité à affiner l’architecture de base de manière itérative nécessite également une plus grande continuité dans le processus. À ce stade, le processus de déploiement implique plusieurs opérations manuelles. Par exemple, l’ajout d’une nouvelle fonctionnalité à l’API nécessiterait de construire une nouvelle image, de la taguer, de mettre à jour les <em>manifests</em> Kubernetes utilisés pour déployer l’API et de les appliquer sur le cluster afin de remplacer l’instance existante avec un temps d’arrêt minimal. De même, un changement de modèle servi via l’API nécessiterait une simple modification du code, mais plusieurs étapes manuelles pour mettre à jour la version sur le cluster. En conséquence, les <em>data scientists</em> ne sont pas totalement autonomes pour prototyper et tester des versions mises à jour du modèle ou de l’API, ce qui limite le potentiel d’amélioration continue.</p>
<p>Afin d’automatiser ce processus, nous avons construit une pipeline CI/CD — un concept déjà présenté dans <a href="#sec-devops-mlops" class="quarto-xref">Section&nbsp;4.2.1</a> — intégrant ces différentes étapes. <a href="#fig-ci-cd" class="quarto-xref">Figure&nbsp;5</a> illustre notre implémentation spécifique de la pipeline CI/CD. Toute modification du code du dépôt de l’API, associée à un nouveau tag, déclenche un processus de build CI (implémenté avec GitHub Actions) de l’image Docker, qui est ensuite publiée sur un hub public de conteneurs (DockerHub). Cette image peut ensuite être récupérée et déployée par l’orchestrateur de conteneurs (Kubernetes) en spécifiant et en appliquant manuellement de nouveaux manifests pour mettre à jour les ressources Kubernetes de l’API.</p>
<p>Cependant, cette approche présente un inconvénient : elle limite la reproductibilité du déploiement, car chaque ressource est gérée indépendamment par l’orchestrateur, et le cycle de vie du déploiement de l’API dans son ensemble n’est pas contrôlé. Pour pallier cette lacune, nous avons intégré la partie déploiement dans une pipeline CD basée sur l’approche GitOps : les <em>manifests</em> des ressources de l’API sont stockés dans un dépôt Git. L’état de ce dépôt “GitOps” est surveillé par un opérateur Kubernetes (ArgoCD), de sorte à ce que toute modification des <em>manifests</em> de l’application soit directement propagée au déploiement sur le cluster. Dans cette pipeline intégrée, la seule action nécessaire pour que le <em>data scientist</em> déclenche une mise à jour de l’API est de modifier le tag de l’image de l’API indiquant la version à déployer.</p>
<div id="fig-ci-cd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/ci-cd.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: La pipeline CI/CD implémentée dans le projet
</figcaption>
</figure>
</div>
</section>
<section id="sec-monitoring" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="sec-monitoring"><span class="header-section-number">4.3.4</span> Surveillance d’un modèle en production</h3>
<p>Une fois la phase initiale de développement du projet terminée — incluant l’entraînement, l’optimisation et le déploiement du modèle pour les utilisateurs —, il est crucial de comprendre que les responsabilités du <em>data scientist</em> ne s’arrêtent pas là. Traditionnellement, le rôle du <em>data scientist</em> se limite souvent à l’entraînement et à la sélection du modèle à déployer, le déploiement étant généralement délégué au département informatique. Cependant, une spécificité des projets de ML est que, une fois en production, le modèle n’a pas encore atteint la fin de son cycle de vie : il doit être surveillé en permanence afin d’éviter toute dégradation indésirable des performances. La surveillance continue du modèle déployé est essentielle pour garantir la conformité des résultats aux attentes, anticiper les changements dans les données et améliorer le modèle de manière itérative. Même en production, les compétences du <em>data scientist</em> sont nécessaires.</p>
<p>Le concept de surveillance peut avoir différentes significations selon le contexte de l’équipe impliquée. Pour les équipes informatiques, il s’agit principalement de vérifier l’efficacité technique de l’application, notamment en termes de latence, de consommation de mémoire ou d’utilisation du disque de stockage. En revanche, pour les <em>data scientists</em> ou les équipes métier, la surveillance est davantage centrée sur le suivi méthodologique du modèle. Cependant, le suivi en temps réel des performances d’un modèle de ML est souvent une tâche complexe, car la vérité terrain (<em>ground truth</em>) n’est généralement pas connue au moment de la prédiction. Il est donc courant d’utiliser des proxys pour détecter les signes éventuels de dégradation des performances. Deux types principaux de dégradation d’un modèle ML sont généralement distingués. Le premier est le <em>data drift</em>, qui se produit lorsque les données utilisées pour l’inférence en production diffèrent significativement des données utilisées lors de l’entraînement. Le second est le <em>concept drift</em>, qui survient lorsqu’un changement dans la relation statistique entre les variables explicatives et la variable cible est observé au fil du temps. Par exemple, le mot “Uber” était habituellement associé à des codes liés aux services de taxis. Cependant, avec l’apparition des services de livraison de repas comme “Uber Eats”, cette relation entre le libellé et le code associé a changé. Il est donc nécessaire de repérer au plus tôt ces changements afin de ne pas dégrader la codification.</p>
<p>Dans le cadre de notre projet, l’objectif est d’atteindre le taux le plus élevé possible de libellés correctement classifiés, tout en minimisant le nombre de descriptions nécessitant une intervention manuelle. Ainsi, notre objectif est de distinguer les prédictions correctes des prédictions incorrectes sans avoir accès au préalable à la vérité terrain. Pour y parvenir, nous calculons un indice de confiance, défini comme la différence entre les deux scores de confiance les plus élevés parmi les résultats renvoyés par le modèle. Pour une description textuelle donnée, si l’indice de confiance dépasse un seuil déterminé, la description est automatiquement codée. Sinon, elle est codée manuellement par un agent de l’Insee. Cette tâche de codification manuel est néanmoins assistée par le modèle ML : via une application qui interroge l’API, l’agent visualise les cinq codes les plus probables selon le modèle. Le seuil choisi pour l’indice de confiance est un paramètre que l’équipe métier peut utiliser pour arbitrer entre la charge de travail qu’elle est disposée à assumer pour la reprise gestionnaire et le taux d’erreurs qu’elle est prête à tolérer.</p>
</section>
<section id="définition-du-seuil-de-codification-automatique-et-surveillance-en-production" class="level3 page-columns page-full" data-number="4.3.5">
<h3 data-number="4.3.5" class="anchored" data-anchor-id="définition-du-seuil-de-codification-automatique-et-surveillance-en-production"><span class="header-section-number">4.3.5</span> Définition du seuil de codification automatique et surveillance en production</h3>
<p>La définition du seuil pour la codification automatique des descriptions textuelles a été une étape cruciale de ce processus, nécessitant un compromis entre un taux élevé de codification automatique et une performance optimale de celle-ci. Pour surveiller le comportement du modèle en production, nous avons développé un tableau de bord interactif permettant de visualiser plusieurs métriques d’intérêt pour les équipes métier. Parmi ces métriques figurent le nombre de requêtes par jour et le taux de codification automatique quotidien, pour un seuil donné pour l’indice de confiance. Cette visualisation permet aux équipes métier de connaître le taux de codification automatique qu’elles auraient obtenu si elles avaient choisi différents seuils. Le tableau de bord représente également la distribution des indices de confiance obtenus et compare des fenêtres temporelles afin de détecter des changements dans les distributions des prédictions renvoyées par le modèle<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Enfin, les indices de confiance peuvent être analysés à des niveaux de granularité plus fins, basés sur les niveaux d’agrégation de la classification statistique, pour identifier les classes les plus difficiles à prédire et celles qui sont plus ou moins fréquentes.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Ces changements de distribution sont généralement vérifiés en calculant des distances statistiques — telles que la distance de Bhattacharyya, la divergence de Kullback-Leibler ou la distance de Hellinger — et/ou en effectuant des tests statistiques — tels que le test de Kolmogorov–Smirnov ou le test du khi-deux.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Idéalement, les frameworks existants devraient être privilégiés par rapport aux solutions sur mesure pour adopter des routines standardisées. Lors de la construction de ce composant du pipeline, nous avons constaté que les frameworks <em>cloud</em> existants pour l’analyse des logs présentaient d’importantes limites. Cela constitue une piste d’amélioration pour le projet.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Successeur de R Markdown, Quarto est devenu un outil essentiel. Il unifie les fonctionnalités de plusieurs packages très utiles de l’écosystème R Markdown tout en offrant une prise en charge native de plusieurs langages de programmation, dont Python et Julia en plus de R. Il est de plus en plus utilisé à l’Insee pour produire des documents reproductibles et les exporter dans divers formats.</p></div></div><p><a href="#fig-full-architecture" class="quarto-xref">Figure&nbsp;6</a> présente les composants ajoutés à l’architecture du projet pour fournir le tableau de bord de surveillance décrit ci-dessus. Tout d’abord, nous avons mis en place un processus simple en Python (deuxième composant de la rangée inférieure), qui récupère quotidiennement les logs de l’API et les transforme en fichiers partitionnés au format Parquet<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Ensuite, nous avons utilisé Quarto<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> pour construire un tableau de bord interactif (troisième composant de la rangée inférieure). Pour calculer les diverses métriques présentées dans le tableau de bord, les fichiers Parquet sont interrogés via le moteur optimisé DuckDB. À l’instar de l’API, le tableau de bord est construit et déployé sous forme de conteneur sur le cluster Kubernetes, et ce processus est également automatisé grâce à une pipeline CI/CD. Le composant d’annotation (quatrième composant de la rangée inférieure) est discuté dans la section suivante.</p>
<div id="fig-full-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-full-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/annotation-datalab.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-full-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Notre implémentation d’une architecture MLOps
</figcaption>
</figure>
</div>
</section>
<section id="sec-annotation" class="level3 page-columns page-full" data-number="4.3.6">
<h3 data-number="4.3.6" class="anchored" data-anchor-id="sec-annotation"><span class="header-section-number">4.3.6</span> Favoriser l’amélioration continue du modèle</h3>
<p>La composante de surveillance de notre modèle fournit une vue détaillée et essentielle de l’activité du modèle en production. En raison de la nature dynamique des données de production, les performances des modèles de ML ont tendance à diminuer avec le temps. Pour favoriser l’amélioration continue du modèle, il est donc essentiel de mettre en place des stratégies permettant de surmonter ces pertes de performance. Une stratégie couramment utilisée est le réentraînement périodique du modèle, nécessitant la collecte de nouvelles données d’entraînement plus récentes et donc plus proches de celle observées en production.</p>
<p>Plusieurs mois après le déploiement de la première version du modèle en production, le besoin de mettre en œuvre un processus d’annotation continue est devenu de plus en plus évident pour deux raisons principales. Premièrement, un échantillon de référence (<em>gold standard</em>) n’était pas disponible lors de la phase d’expérimentation. Nous avons donc utilisé un sous-ensemble des données d’entraînement pour l’évaluation, tout en sachant que la qualité de la labelisation n’était pas optimale. La collecte continue d’un échantillon de référence permettrait ainsi d’obtenir une vue réaliste des performances du modèle en production sur des données réelles, en particulier sur les données codifiées automatiquement. Deuxièmement, la refonte de la nomenclature statistique APE prévue en 2025 impose aux INS d’adopter la dernière version. Cette révision, qui introduit des changements importants, nécessite une adaptation du modèle et surtout la création d’un nouveau jeu de données d’entraînement. L’annotation de l’ancien jeu de données d’entraînement selon la nouvelle nomenclature statistique est donc indispensable.</p>
<p>Dans ce contexte, une campagne d’annotation a été lancée début 2024 pour construire de manière continue un jeu de données de référence. Cette campagne est réalisée sur le SSP Cloud<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> en utilisant le service Label Studio, un outil open source d’annotation offrant une interface ergonomique et disponible dans le catalogue d’Onyxia. <a href="#fig-full-architecture" class="quarto-xref">Figure&nbsp;6</a> montre comment le composant d’annotation (quatrième composant de la rangée inférieure) a pu être intégré facilement dans l’architecture du projet grâce à sa nature modulaire. En pratique, un échantillon de descriptions textuelles est tiré aléatoirement des données passées par l’API au cours des trois derniers mois. Cet échantillon est ensuite soumis à l’annotation par des experts APE via l’interface de Label Studio. Les résultats de l’annotation sont automatiquement sauvegardés sur MinIO, transformés au format Parquet, puis intégrés directement dans le tableau de bord de surveillance pour calculer et observer diverses métriques de performance du modèle. Ces métriques offrent une vision beaucoup plus précise des performances réelles du modèle sur les données de production, et permet notamment de détecter les cas les plus problématiques.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<a href="">https://datalab.sspcloud.fr/</a></p></div></div><p>En parallèle, une campagne d’annotation pour construire un nouveau jeu d’entraînement adapté à la NAF 2025 a également été réalisée. En exploitant à la fois les nouvelles données d’entraînement et les métriques de performance dérivées de l’échantillon de référence, nous visons à améliorer la précision du modèle de manière itérative grâce à des réentraînements périodiques et automatique dès lors que le moteur de codification aura migré sur le cluster Kubernetes de production.</p>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-joulin2016bag" class="csl-entry" role="listitem">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. <span>“Bag of Tricks for Efficient Text Classification.”</span> <em>arXiv Preprint arXiv:1607.01759</em>.
</div>
<div id="ref-li2022survey" class="csl-entry" role="listitem">
Li, Qian, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S Yu, and Lifang He. 2022. <span>“A Survey on Text Classification: From Traditional to Deep Learning.”</span> <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 13 (2): 1–41.
</div>
<div id="ref-meyer_sicore_1997" class="csl-entry" role="listitem">
Meyer, Eric, and Pascal Rivière. 1997. <span>“<span>SICORE</span>, Un Outil Et Une Méthode Pour Le Chiffrement Automatique à <span>I</span>’<span>INSEE</span>.”</span> In <em>Actes de La 4ème <span>Conférence</span> <span>Internationale</span> Des <span>Utilisateurs</span> de <span>Blaise</span></em>, 280–93. Paris, France. <a href="http://www.blaiseusers.org/1997/papers/meyer97.pdf">http://www.blaiseusers.org/1997/papers/meyer97.pdf</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/thomasfaria\.github\.io\/retex-innovation-insee");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../src/implementation/index.html" class="pagination-link" aria-label="3 - Implémentation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Implémentation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../src/discussion/index.html" class="pagination-link" aria-label="5 - Discussion">
        <span class="nav-page-text">5 - Discussion</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>